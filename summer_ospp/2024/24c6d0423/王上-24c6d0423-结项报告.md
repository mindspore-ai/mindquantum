<h1 align = "center">王上-24c6d0423-结项报告</h1>

## 1 项目信息

### 1.1 项目名称

量子启发式算法的GPU/NPU加速

### 1.2 结项材料结构

<img src="figures\struct.png" style="zoom:75%;" />

### 1.3 方案描述

工作概括：
- 进行 BSB/DSB 算法在低精度格式下的可行性验证
- 基于 CUDA 开发高效算子
- 将 GPU 算子封装到MindQuantum 框架中。

我将从这三方面讲解我的工作。

#### 1.3.1 BSB/DSB 算法低精格式可行性验证

**First Take Aways** : BSB/DSB 用于解决最大割问题时选 INT8 格式可以获得最极致加速，其他情况选择 FP16 格式。

- **实验一**：探索 BSB/DSB 用于解决最大割问题时低精加速造成的质量损失。

  实验结果如下（注：质量损失率为负表示求解质量比高精度格式求解更好）。可以发现质量损失在千分之一这个数量级，满足要求，甚至有相当多的情况求解质量超过 baseline 。

![image-20240923162307170](figures\image-20240923162307170.png)

- **实验二**：探索当耦合矩阵和外场强度矩阵的元素都是服从独立正态分布的实数时，低精求解的质量损失情况。

  实验结果如下。可见当耦合矩阵和外场强度矩阵的元素都是服从独立正态分布的实数时，使用 INT8 格式求解导致质量损失过大，FP16 符合要求。

  ![image-20240923164650992](figures\image-20240923164650992.png)

#### 1.3.2 CUDA 算子实现

**整体流程如图：**

![image-20240923141630628](figures\overview.png)

 **Second Take Aways**：batch size 较大，加速更明显。

与原版 CPU 算子性能对比实验如下。

**实验环境**：

- 24核的 i9-13900HX CPU用于跑原版CPU算子
- NVIDIA 3090
- Docker container : swr.cn-south-1.myhuaweicloud.com/mindspore/mindspore-gpu-cuda11.6:2.2.14
- Dataset : G22

**实验结果**：下图中分别展示了 BSB/DSB 的 GPU 算子对比原版的加速比，性能大幅提升。注：batch size 为 spin value 矩阵的列数。FP16 格式在保证一定的精度前提下，拥有可观的加速效果。



![image-20240923170947945](figures\image-20240923170947945.png)

在不同数据集上加速效果：

![image-20240924152534626](figures\image-20240924152534626.png)

#### 1.3.3 MindQuantum GPU 算子封装

- 在 ccsrc/ 下 include/ , lib/ , python/ 中分别实现头文件定义、具体实现、封装到python。

- 在 mindquantum/algorithm/qaia/SB.py 中 import 自定义的库，并根据执行逻辑调用对应的算子。

- 在 setup.py 中注册算子。

调用示例：

<img src="figures\image-20240924152328088.png" alt="image-20240924152328088" style="zoom: 70%;" />

**测试结果**(测试脚本见test_qaia.py)：

<img src="figures\test_res.png" style="zoom:67%;" />

**运行测试脚本**需要满足以下条件：

1. 安装指定版本的 mindquantum ，流程如下。

- 获取我的项目：

  ```
  git clone https://gitee.com/shannonwand/mindquantum.git
  ```

- 在满足要求的环境中编译并 build（本人开发环境：swr.cn-south-1.myhuaweicloud.com/mindspore/mindspore-gpu-cuda11.6:2.2.14），进入 mindquantum 执行：

  ```
  bash build.sh --gitee --gpu
  ```

- 安装刚刚构建的 whl 包：

  ```
  pip install output/mindquantum-0.10.0-cp37-cp37m-manylinux_2_27_x86_64.whl --force-reinstall
  ```

2. 下载好 Gset 数据集，与测试脚本放在同一文件夹中。

### 1.4 时间线概览

| 时间 |                             内容                             |
| :--: | :----------------------------------------------------------: |
| 7月  | ①简单验证低精优化 BSB/DSB 算法的可行性。②梳理 BSB/DSB 算法的 workflow。③跑通基于 Tensor Cores 实现的 BSB/DSB 算法流程。 |
| 8月  | ①探索 MindQuantum GPU 算子封装流程。②整理 MindQuantum GPU 算子封装教程文档。 |
| 9月  | ①补充实验进一步讨论不同低精格式在 BSB/DSB 算法的不同应用场景的适用条件。②跑性能对比实验。③整理资料完成项目展示PPT。 |

## 2 项目进度

- 已完成工作：

  - BSB/DSB 低精加速可行性验证。**结论**：BSB/DSB 在求解最大割问题时选择 INT8 精度可获得极致加速（几乎没有求解质量损失），其他场景建议使用 FP16 格式。

  - 基于 NVIDIA Tensor Cores 实现了支持 INT8 和 FP16 精度的 BSB/DSB 算子。
  - 将基于 GPU 自定义 BSB/DSB 算子封装到 MindQuantum 框架。
  - 将探索的 GPU 算子封装流程整理为教程文档，降低 MindQuantum GPU 算子开发/集成门槛。

- 遇到的问题及解决方案：

  - **遇到问题**：没有现成的 MindQuantum GPU 算子封装教程。**解决**：根据已有的 CMakeLists 和 build 脚本梳理编译流程。打印日志，根据报错信息推导问题来源。
  - **遇到问题**：不清楚算法的背景以及不同的应用场景对应的不同数据特性，影响实验。**解决**：与导师沟通。阅读相关论文，了解算法的应用场景和数据特性。根据不同场景的数据特性生成实验数据进行实验验证低精加速可行性。

- 后续工作安排：探索优化模拟分岔算法以实现更低精度支持。

