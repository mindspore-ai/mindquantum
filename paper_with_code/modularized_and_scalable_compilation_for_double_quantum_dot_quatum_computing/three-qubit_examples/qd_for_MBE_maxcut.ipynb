{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b1c81b",
   "metadata": {},
   "source": [
    "# 基于 MindQuantum 0.7.0 实现半导体双量子点下，采用多基矢编码方法 MBE 实现最大割求解\n",
    "\n",
    "## 2-qubit QD MBE-VQE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from numpy import kron\n",
    "from mindquantum import *\n",
    "from scipy.linalg import expm\n",
    "from mindspore.ops import operations\n",
    "from mindspore import nn, ops, Tensor, context\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.common.initializer import initializer  \n",
    "from mindspore.nn import Adam, TrainOneStepCell, LossBase\n",
    "ms.context.set_context(mode=ms.context.PYNATIVE_MODE, device_target=\"CPU\")\n",
    "ms.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "s_x = X.matrix()\n",
    "s_z = Z.matrix()\n",
    "one = I.matrix()\n",
    "dt = np.pi/2\n",
    "ddt = np.pi/10\n",
    "\n",
    "def _matrix_(coeff):\n",
    "    return expm(-1j*(coeff*s_z+s_x)*dt)\n",
    "\n",
    "def _diff_matrix_(coeff):\n",
    "    return -1j*_matrix_(coeff)@(s_z*dt)\n",
    "\n",
    "def _matrix_0(coeff):\n",
    "    return expm(-1j*(coeff*s_z+s_x)*ddt)\n",
    "\n",
    "def _diff_matrix_0(coeff):\n",
    "    return -1j*_matrix_0(coeff)@(s_z*ddt)\n",
    "\n",
    "def _matrix_c_0(coeff):\n",
    "    return expm(-1j*(coeff*kron(s_z, one) + kron(one, s_z) + kron(s_x, one) + kron(one, s_x) + coeff*kron(s_z-one, s_z-one))*5*ddt)\n",
    "\n",
    "def _diff_matrix_c_0(coeff):\n",
    "    return -1j*_matrix_c_0(coeff)@((kron(s_z, one) + kron(s_z-one, s_z-one)) * 5*ddt)\n",
    "\n",
    "def _matrix_c_1(coeff):\n",
    "    return expm(-1j*(kron(s_z, one) + coeff*kron(one, s_z) + kron(s_x, one) + kron(one, s_x) + coeff*kron(s_z-one, s_z-one))*5*ddt)\n",
    "\n",
    "def _diff_matrix_c_1(coeff):\n",
    "    return -1j*_matrix_c_1(coeff)@((kron(one, s_z) + kron(s_z-one, s_z-one)) *  5*ddt)\n",
    "\n",
    "gate = gene_univ_parameterized_gate('gete', _matrix_, _diff_matrix_) # dt=pi/2\n",
    "gate_0 = gene_univ_parameterized_gate('gete_0', _matrix_0, _diff_matrix_0) # ddt=pi/10\n",
    "gate_c_0 = gene_univ_parameterized_gate('gete_c_0', _matrix_c_0, _diff_matrix_c_0) # ddt=pi/10\n",
    "gate_c_1 = gene_univ_parameterized_gate('gete_c_1', _matrix_c_1, _diff_matrix_c_1) # ddt=pi/10\n",
    "\n",
    "cz_params = np.array( [1.5472503,  1.4179231,  1.540713,   1.9724044,  1.9253408,  1.3879265,\n",
    "                     0.8130467,  0.76446086, 1.2703444,  1.8553745,  1.0291328,  1.2492974,\n",
    "                     0.7880994,  0.3026381,  0.31203356, 0.30834132, 0.9533752,  1.3802187,\n",
    "                     1.270656,   0.5646567,  0.94619316, 0.97377133, 1.9658349,  0.83277696,\n",
    "                     1.0190777,  0.90001523, 0.26008993, 0.16526282, 0.22249524, 1.1596956,\n",
    "                     1.5285202,  0.4919534,  0.01645389, 0.02608137, 0.6504683,  0.31325826,\n",
    "                     0.4486266,  0.8677286,  1.3571227,  1.4995408,  1.1248059,  0.5996333,\n",
    "                     0.32797617, 0.54987127])\n",
    "# params_name = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '0', '1', '2', '3', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119']\n",
    "\n",
    "def cz_circ():\n",
    "    circ_ = Circuit()\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate_0(param).on(0) for param in cz_params[:10]])\n",
    "    circ_ += Circuit([gate_0(0).on(1) for i in range(10)])\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate_0(0).on(0) for i in range(10)])\n",
    "    circ_ += Circuit([gate_0(param).on(1) for param in cz_params[10:20]])\n",
    "    circ_ += Circuit([gate_c_0(cz_params[20]).on([1,0]), gate_c_0(cz_params[21]).on([1,0])])\n",
    "    circ_ += Circuit([gate_c_1(cz_params[22]).on([1,0]), gate_c_1(cz_params[23]).on([1,0])])\n",
    "    circ_ += Circuit([gate_0(param).on(0) for param in cz_params[24:34]])\n",
    "    circ_ += Circuit([gate_0(0).on(1) for i in range(10,20)])\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate_0(0).on(0) for i in range(10)])\n",
    "    circ_ += Circuit([gate_0(param).on(1) for param in cz_params[34:]])\n",
    "    circ_ += BarrierGate()\n",
    "    return circ_\n",
    "\n",
    "def arb_circ(prefix='0'):\n",
    "    circ_ = Circuit()\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate(f'arb0{i}').on(0) for i in range(12)])\n",
    "    circ_ += Circuit([gate(0).on(1) for i in range(12)])\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate(0).on(0) for i in range(12)])\n",
    "    circ_ += Circuit([gate(f'arb1{i}').on(1) for i in range(12)])\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ = add_prefix(circ_, prefix)\n",
    "    return circ_\n",
    "\n",
    "def qlayer(prefix='0'):\n",
    "    circ_ = Circuit()\n",
    "    circ_ += arb_circ(prefix)\n",
    "    circ_ += cz_circ()\n",
    "    return circ_\n",
    "\n",
    "class my_simulator: \n",
    "    def get_expectation_with_grad(self, hams, circ): # 输入为 hams, circ_right, simulator_left 等，这里就只用 hams 代替。\n",
    "        sim = Simulator('projectq', 2) \n",
    "        \n",
    "        def grad_ops(inputs): # 输入为各量子门的参数\n",
    "            h = 1e-4\n",
    "            grad = []\n",
    "            sim.reset()\n",
    "            sim.apply_circuit(circ, inputs)\n",
    "            exceptation = [sim.get_expectation(ham).real for ham in hams]\n",
    "            for i in range(inputs.size):\n",
    "                params_p, params_n = copy.deepcopy(inputs), copy.deepcopy(inputs)\n",
    "                params_p[i] += h\n",
    "                params_n[i] -= h\n",
    "                sim.reset()\n",
    "                sim.apply_circuit(circ, params_p)\n",
    "                g_p = [sim.get_expectation(ham) for ham in hams]\n",
    "                sim.reset()\n",
    "                sim.apply_circuit(circ, params_n)\n",
    "                g_n = [sim.get_expectation(ham) for ham in hams]\n",
    "                grad.append([(p.real - n.real)/(2*h) for p, n in zip(g_p, g_n)])\n",
    "                sim.reset()\n",
    "            return exceptation, np.array(grad).T # 运行结果为期望值和梯度\n",
    "        return grad_ops # MindQuantum 中，这里返回的是一个封装了 hams, grad_ops 等的一个封装器，这里简化为单独的 grad_ops\n",
    "\n",
    "class ansatz_only_ops(nn.Cell):\n",
    "    def __init__(self, expectation_with_grad):\n",
    "        super().__init__()\n",
    "        self.expectation_with_grad = expectation_with_grad\n",
    "        self.shape_ops = operations.Shape()\n",
    "        self.g = None \n",
    "\n",
    "    def construct(self, arg):\n",
    "        fval, g_ans = self.expectation_with_grad(arg.asnumpy())\n",
    "        self.g = np.real(g_ans)\n",
    "        return ms.Tensor(np.real(fval), dtype=ms.float32)\n",
    "\n",
    "    def bprop(self, arg, out, dout): \n",
    "        dout = dout.asnumpy()\n",
    "        grad = dout @ self.g\n",
    "        return ms.Tensor(grad, dtype=ms.float32)\n",
    "\n",
    "class ansatz_only_layer(nn.Cell):\n",
    "    def __init__(self, expectation_with_grad, weight='ones'):\n",
    "        super().__init__()\n",
    "        self.evolution = ansatz_only_ops(expectation_with_grad)\n",
    "        weight_size = len(circ.params_name)\n",
    "        self.weight = Parameter(initializer(weight, weight_size, dtype=ms.float32), name='ansatz_weight')\n",
    "        self.abs = ops.Abs()\n",
    "\n",
    "    def construct(self):\n",
    "        return self.evolution(self.abs(self.weight))\n",
    "\n",
    "class MyLoss(LossBase):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(MyLoss, self).__init__(reduction)\n",
    "        self.tanh = ops.Tanh()\n",
    "\n",
    "    def construct(self, logits):\n",
    "        out = self.tanh(logits)\n",
    "        out = out[0] * out[2] + out[1] * out[3] + out[0] * out[1] \n",
    "        return self.get_loss(out)\n",
    "\n",
    "class MyWithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, loss_fn):\n",
    "       super(MyWithLossCell, self).__init__(auto_prefix=False)\n",
    "       self._backbone = backbone\n",
    "       self._loss_fn = loss_fn\n",
    "\n",
    "    def construct(self):\n",
    "       out = self._backbone()\n",
    "       return self._loss_fn(out)\n",
    "\n",
    "    @property\n",
    "    def backbone_network(self):\n",
    "       return self._backbone\n",
    "\n",
    "\n",
    "circ = Circuit()\n",
    "for i in range(2):\n",
    "    circ += qlayer(prefix=f'{i}')\n",
    "\n",
    "hams = [Hamiltonian(QubitOperator('Z0')), Hamiltonian(QubitOperator('X0')), \n",
    "        Hamiltonian(QubitOperator('Z1')), Hamiltonian(QubitOperator('X1'))]\n",
    "\n",
    "my_sim = my_simulator()\n",
    "grad_ops = my_sim.get_expectation_with_grad(hams, circ)\n",
    "\n",
    "qnet = ansatz_only_layer(grad_ops)\n",
    "loss = MyLoss()\n",
    "net_with_criterion = MyWithLossCell(qnet, loss)\n",
    "opti = Adam(qnet.trainable_params(), learning_rate=0.1) \n",
    "net = TrainOneStepCell(net_with_criterion, opti)\n",
    "\n",
    "round = ops.Round()\n",
    "loss_list = []\n",
    "res_list = []\n",
    "for i in range(100):\n",
    "    loss = net()\n",
    "    loss_list.append(loss)\n",
    "    print(f'\\n当前训练次数为：{i}, 损失函数值为：{loss}')\n",
    "    out = qnet()\n",
    "    res = (1 - round(out[0]) * round(out[2])) / 2 + (1 - round(out[1]) * round(out[3])) / 2 + (1 - round(out[0]) * round(out[1])) / 2\n",
    "    print('MBE算法计算的最大割数为：', res) #int(res + 0.5))\n",
    "    res_list.append(res) #int(res + 0.5))\n",
    "    \n",
    "print('\\nloss_list:\\n', loss_list)\n",
    "print('\\nres_list:\\n', res_list)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46371af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_array = np.array([])\n",
    "for i in loss_list:\n",
    "    loss_array = np.append(loss_array, i.asnumpy())\n",
    "    \n",
    "res_array = np.array([])\n",
    "for i in res_list:\n",
    "    res_array = np.append(res_array, i.asnumpy())\n",
    "    \n",
    "print('loss_array\\n', loss_array)\n",
    "print('\\nres_array\\n', res_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f69fe",
   "metadata": {},
   "source": [
    "0.56820911 -0.06772007 -0.15884987  0.21334288  0.24638289 -0.01383337 -0.64901042 -0.77732551 -0.9761011  -0.83940554 -0.78345203 -0.81188357 -0.77375078 -0.87277997 -0.88260961 -0.92767239 -1.0147531  -1.0166626 -1.07208765 -1.0819068  -1.03471255 -1.03653717 -1.08419538 -1.09106088 -1.06723678 -1.06724095 -1.06764245 -1.09123957 -1.10459733 -1.07034826 -1.08677888 -1.09873986 -1.09860384 -1.09767449 -1.08537626 -1.09996045 -1.10696483 -1.09967256 -1.10074282 -1.10184681 -1.1062355  -1.10451984 -1.10361683 -1.10871792 -1.10590696 -1.10561097 -1.10770726 -1.10933506 -1.10946894 -1.10612607 -1.11011922 -1.10973358 -1.1099844  -1.1090343 -1.11044478 -1.11054432 -1.10974407 -1.11115432 -1.11102438 -1.1106931 -1.11028945 -1.111848   -1.11140621 -1.11092186 -1.11115718 -1.11181331 -1.11158633 -1.11131036 -1.11175632 -1.11166263 -1.11168683 -1.11176801 -1.11192179 -1.11164129 -1.11189651 -1.11194324 -1.1119287  -1.11181188 -1.11202347 -1.11194563 -1.11198425 -1.11199236 -1.11201954 -1.11195874 -1.11209655 -1.11203265 -1.11201501 -1.112046   -1.11211503 -1.11203694 -1.11206317 -1.11209905 -1.11207974 -1.11208487 -1.11209893 -1.11209238 -1.11209142 -1.11211896 -1.11209583 -1.11210334"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b6c1a",
   "metadata": {},
   "source": [
    "1.5 1.5 1.5 1.5 1.5 2.  2.5 3.  2.  2.5 2.5 2.  2.  2.5 2.5 2.5 2.  3. 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3. 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3. 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3. 3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3. 3.  3.  3.  3.  3.  3.  3.  3.  3.  3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc121f99",
   "metadata": {},
   "source": [
    "## 3-qubit QD MBE-VQE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3483ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from numpy import kron\n",
    "from mindquantum import *\n",
    "from scipy.linalg import expm\n",
    "from mindspore.ops import operations\n",
    "from mindspore import nn, ops, Tensor, context\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.common.initializer import initializer  \n",
    "from mindspore.nn import Adam, TrainOneStepCell, LossBase\n",
    "ms.context.set_context(mode=ms.context.PYNATIVE_MODE, device_target=\"CPU\")\n",
    "ms.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "s_x = X.matrix()\n",
    "s_z = Z.matrix()\n",
    "one = I.matrix()\n",
    "dt = np.pi/2\n",
    "ddt = np.pi/10\n",
    "\n",
    "def _matrix_(coeff):\n",
    "    return expm(-1j*(coeff*s_z+s_x)*dt)\n",
    "\n",
    "def _diff_matrix_(coeff):\n",
    "    return -1j*_matrix_(coeff)@(s_z*dt)\n",
    "\n",
    "def _matrix_0(coeff):\n",
    "    return expm(-1j*(coeff*s_z+s_x)*ddt)\n",
    "\n",
    "def _diff_matrix_0(coeff):\n",
    "    return -1j*_matrix_0(coeff)@(s_z*ddt)\n",
    "\n",
    "def _matrix_c_0(coeff):\n",
    "    return expm(-1j*(coeff*kron(s_z, one) + kron(one, s_z) + kron(s_x, one) + kron(one, s_x) + coeff*kron(s_z-one, s_z-one))*5*ddt)\n",
    "\n",
    "def _diff_matrix_c_0(coeff):\n",
    "    return -1j*_matrix_c_0(coeff)@((kron(s_z, one) + kron(s_z-one, s_z-one)) * 5*ddt)\n",
    "\n",
    "def _matrix_c_1(coeff):\n",
    "    return expm(-1j*(kron(s_z, one) + coeff*kron(one, s_z) + kron(s_x, one) + kron(one, s_x) + coeff*kron(s_z-one, s_z-one))*5*ddt)\n",
    "\n",
    "def _diff_matrix_c_1(coeff):\n",
    "    return -1j*_matrix_c_1(coeff)@((kron(one, s_z) + kron(s_z-one, s_z-one)) *  5*ddt)\n",
    "\n",
    "gate = gene_univ_parameterized_gate('gete', _matrix_, _diff_matrix_) # dt=pi/2\n",
    "gate_0 = gene_univ_parameterized_gate('gete_0', _matrix_0, _diff_matrix_0) # ddt=pi/10\n",
    "gate_c_0 = gene_univ_parameterized_gate('gete_c_0', _matrix_c_0, _diff_matrix_c_0) # ddt=pi/10\n",
    "gate_c_1 = gene_univ_parameterized_gate('gete_c_1', _matrix_c_1, _diff_matrix_c_1) # ddt=pi/10\n",
    "\n",
    "cz_params = np.array( [1.5472503,  1.4179231,  1.540713,   1.9724044,  1.9253408,  1.3879265,\n",
    "                     0.8130467,  0.76446086, 1.2703444,  1.8553745,  1.0291328,  1.2492974,\n",
    "                     0.7880994,  0.3026381,  0.31203356, 0.30834132, 0.9533752,  1.3802187,\n",
    "                     1.270656,   0.5646567,  0.94619316, 0.97377133, 1.9658349,  0.83277696,\n",
    "                     1.0190777,  0.90001523, 0.26008993, 0.16526282, 0.22249524, 1.1596956,\n",
    "                     1.5285202,  0.4919534,  0.01645389, 0.02608137, 0.6504683,  0.31325826,\n",
    "                     0.4486266,  0.8677286,  1.3571227,  1.4995408,  1.1248059,  0.5996333,\n",
    "                     0.32797617, 0.54987127])\n",
    "# params_name = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '0', '1', '2', '3', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119']\n",
    "\n",
    "def cz_circ(obj_qubit, ctrl_qubit):\n",
    "    circ_ = Circuit()\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate_0(param).on(ctrl_qubit) for param in cz_params[:10]])\n",
    "    # circ_ += Circuit([gate_0(0).on(obj_qubit) for i in range(10)])\n",
    "    circ_ += BarrierGate()\n",
    "    # circ_ += Circuit([gate_0(0).on(ctrl_qubit) for i in range(10)])\n",
    "    circ_ += Circuit([gate_0(param).on(obj_qubit) for param in cz_params[10:20]])\n",
    "    circ_ += Circuit([gate_c_0(cz_params[20]).on([obj_qubit,ctrl_qubit]), gate_c_0(cz_params[21]).on([obj_qubit,ctrl_qubit])])\n",
    "    circ_ += Circuit([gate_c_1(cz_params[22]).on([obj_qubit,ctrl_qubit]), gate_c_1(cz_params[23]).on([obj_qubit,ctrl_qubit])])\n",
    "    circ_ += Circuit([gate_0(param).on(ctrl_qubit) for param in cz_params[24:34]])\n",
    "    # circ_ += Circuit([gate_0(0).on(obj_qubit) for i in range(10,20)])\n",
    "    circ_ += BarrierGate()\n",
    "    # circ_ += Circuit([gate_0(0).on(ctrl_qubit) for i in range(10)])\n",
    "    circ_ += Circuit([gate_0(param).on(obj_qubit) for param in cz_params[34:]])\n",
    "    circ_ += BarrierGate()\n",
    "    return circ_\n",
    "\n",
    "def arb_circ(prefix='0'):\n",
    "    circ_ = Circuit()\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate(f'arb0{i}').on(0) for i in range(12)])\n",
    "    # circ_ += Circuit([gate(0).on(1) for i in range(12)])\n",
    "    # circ_ += Circuit([gate(0).on(2) for i in range(12)])\n",
    "    circ_ += BarrierGate()\n",
    "    # circ_ += Circuit([gate(0).on(0) for i in range(12)])\n",
    "    circ_ += Circuit([gate(f'arb1{i}').on(1) for i in range(12)])\n",
    "    # circ_ += Circuit([gate(0).on(2) for i in range(12)])\n",
    "    circ_ += BarrierGate()\n",
    "    # circ_ += Circuit([gate(0).on(0) for i in range(12)])\n",
    "    # circ_ += Circuit([gate(0).on(1) for i in range(12)])\n",
    "    circ_ += Circuit([gate(f'arb2{i}').on(2) for i in range(12)])\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ = add_prefix(circ_, prefix)\n",
    "    return circ_\n",
    "\n",
    "def Cz_circ():\n",
    "    circ_ = Circuit()\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate_0(param).on(0) for param in cz_params[:10]])\n",
    "    circ_ += Circuit([gate_0(0).on(1) for i in range(10)])\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate_0(0).on(0) for i in range(10)])\n",
    "    circ_ += Circuit([gate_0(param).on(1) for param in cz_params[10:20]])\n",
    "    circ_ += Circuit([gate_c_0(cz_params[20]).on([1,0]), gate_c_0(cz_params[21]).on([1,0])])\n",
    "    circ_ += Circuit([gate_c_1(cz_params[22]).on([1,0]), gate_c_1(cz_params[23]).on([1,0])])\n",
    "    circ_ += Circuit([gate_0(param).on(0) for param in cz_params[24:34]])\n",
    "    circ_ += Circuit([gate_0(0).on(1) for i in range(10,20)])\n",
    "    circ_ += BarrierGate()\n",
    "    circ_ += Circuit([gate_0(0).on(0) for i in range(10)])\n",
    "    circ_ += Circuit([gate_0(param).on(1) for param in cz_params[34:]])\n",
    "    circ_ += BarrierGate()\n",
    "    return circ_\n",
    "\n",
    "def qlayer(prefix='0'):\n",
    "    circ_ = Circuit()\n",
    "    circ_ += arb_circ('0'+prefix)\n",
    "    circ_ += cz_circ(1, 0)\n",
    "    # circ_ += Circuit([gate_0(0).on(2) for i in range(10)])\n",
    "    # circ_ += arb_circ('1'+prefix)\n",
    "    circ_ += cz_circ(2, 1)\n",
    "    # circ_ += Circuit([gate_0(0).on(0) for i in range(10)])\n",
    "    # circ_ += cz_circ(0, 2)\n",
    "    # circ_ += Circuit([gate_0(0).on(1) for i in range(10)])\n",
    "    return circ_\n",
    "\n",
    "class my_simulator: \n",
    "    def get_expectation_with_grad(self, hams, circ): # 输入为 hams, circ_right, simulator_left 等，这里就只用 hams 代替。\n",
    "        sim = Simulator('projectq', 3) \n",
    "        \n",
    "        def grad_ops(inputs): # 输入为各量子门的参数\n",
    "            h = 1e-4\n",
    "            grad = []\n",
    "            sim.reset()\n",
    "            sim.apply_circuit(circ, inputs)\n",
    "            exceptation = [sim.get_expectation(ham).real for ham in hams]\n",
    "            for i in range(inputs.size):\n",
    "                params_p, params_n = copy.deepcopy(inputs), copy.deepcopy(inputs)\n",
    "                params_p[i] += h\n",
    "                params_n[i] -= h\n",
    "                sim.reset()\n",
    "                sim.apply_circuit(circ, params_p)\n",
    "                g_p = [sim.get_expectation(ham) for ham in hams]\n",
    "                sim.reset()\n",
    "                sim.apply_circuit(circ, params_n)\n",
    "                g_n = [sim.get_expectation(ham) for ham in hams]\n",
    "                grad.append([(p.real - n.real)/(2*h) for p, n in zip(g_p, g_n)])\n",
    "                sim.reset()\n",
    "            return exceptation, np.array(grad).T # 运行结果为期望值和梯度\n",
    "        return grad_ops # MindQuantum 中，这里返回的是一个封装了 hams, grad_ops 等的一个封装器，这里简化为单独的 grad_ops\n",
    "\n",
    "class ansatz_only_ops(nn.Cell):\n",
    "    def __init__(self, expectation_with_grad):\n",
    "        super().__init__()\n",
    "        self.expectation_with_grad = expectation_with_grad\n",
    "        self.shape_ops = operations.Shape()\n",
    "        self.g = None \n",
    "\n",
    "    def construct(self, arg):\n",
    "        fval, g_ans = self.expectation_with_grad(arg.asnumpy())\n",
    "        self.g = np.real(g_ans)\n",
    "        return ms.Tensor(np.real(fval), dtype=ms.float32)\n",
    "\n",
    "    def bprop(self, arg, out, dout): \n",
    "        dout = dout.asnumpy()\n",
    "        grad = dout @ self.g\n",
    "        return ms.Tensor(grad, dtype=ms.float32)\n",
    "\n",
    "class ansatz_only_layer(nn.Cell):\n",
    "    def __init__(self, expectation_with_grad, weight='ones'):\n",
    "        super().__init__()\n",
    "        self.evolution = ansatz_only_ops(expectation_with_grad)\n",
    "        weight_size = len(circ.params_name)\n",
    "        self.weight = Parameter(initializer(weight, weight_size, dtype=ms.float32), name='ansatz_weight')\n",
    "        self.abs = ops.Abs()\n",
    "\n",
    "    def construct(self):\n",
    "        return self.evolution(self.abs(self.weight))\n",
    "\n",
    "class MyLoss(LossBase):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(MyLoss, self).__init__(reduction)\n",
    "        self.tanh = ops.Tanh()\n",
    "\n",
    "    def construct(self, logits):\n",
    "        x = self.tanh(logits)\n",
    "        out = 0\n",
    "        for edge in edges:\n",
    "            out += x[edge[0]] * x[edge[1]]\n",
    "        return self.get_loss(out)\n",
    "\n",
    "class MyWithLossCell(nn.Cell):\n",
    "    def __init__(self, backbone, loss_fn):\n",
    "       super(MyWithLossCell, self).__init__(auto_prefix=False)\n",
    "       self._backbone = backbone\n",
    "       self._loss_fn = loss_fn\n",
    "\n",
    "    def construct(self):\n",
    "       out = self._backbone()\n",
    "       return self._loss_fn(out)\n",
    "\n",
    "    @property\n",
    "    def backbone_network(self):\n",
    "       return self._backbone\n",
    "\n",
    "edges = []\n",
    "node_num = 6\n",
    "for node in range(node_num-1): \n",
    "    edges.append([node, node+1])\n",
    "edges.append([node_num-1, 0])\n",
    "for node in range(int(node_num/2)):\n",
    "    edges.append([node, int(node+node_num/2)])\n",
    "print(edges)\n",
    "circ = Circuit()\n",
    "for i in range(2):\n",
    "    circ += qlayer(prefix=f'{i}')\n",
    "\n",
    "hams = [Hamiltonian(QubitOperator('Z0')), Hamiltonian(QubitOperator('X0')), \n",
    "        Hamiltonian(QubitOperator('Z1')), Hamiltonian(QubitOperator('X1')),\n",
    "        Hamiltonian(QubitOperator('Z2')), Hamiltonian(QubitOperator('X2'))]\n",
    "\n",
    "my_sim = my_simulator()\n",
    "grad_ops = my_sim.get_expectation_with_grad(hams, circ)\n",
    "\n",
    "qnet = ansatz_only_layer(grad_ops)\n",
    "loss = MyLoss()\n",
    "net_with_criterion = MyWithLossCell(qnet, loss)\n",
    "opti = Adam(qnet.trainable_params(), learning_rate=0.1) \n",
    "net = TrainOneStepCell(net_with_criterion, opti)\n",
    "\n",
    "round = ops.Round()\n",
    "loss_list = []\n",
    "res_list = []\n",
    "for i in range(100):\n",
    "    loss = net()\n",
    "    loss_list.append(loss)\n",
    "    print(f'\\n当前训练次数为：{i}, 损失函数值为：{loss}')\n",
    "    out = qnet()\n",
    "    result = 0\n",
    "    for edge in edges:\n",
    "        result += (1 - round(out[edge[0]]) * round(out[edge[1]])) / 2\n",
    "    print('MBE算法计算的最大割数为：', int(result.asnumpy() + 0.5)) #int(res + 0.5))\n",
    "    res_list.append(int(result.asnumpy() + 0.5)) #int(res + 0.5))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100,201):\n",
    "#     loss = net()\n",
    "#     loss_list.append(loss)\n",
    "#     print(f'\\n当前训练次数为：{i}, 损失函数值为：{loss}')\n",
    "#     out = qnet()\n",
    "#     result = 0\n",
    "#     for edge in edges:\n",
    "#         result += (1 - round(out[edge[0]]) * round(out[edge[1]])) / 2\n",
    "#     print('MBE算法计算的最大割数为：', int(result.asnumpy() + 0.5)) #int(res + 0.5))\n",
    "#     res_list.append(int(result.asnumpy() + 0.5)) #int(res + 0.5))\n",
    "    \n",
    "loss_array = np.array([])\n",
    "for i in loss_list:\n",
    "    loss_array = np.append(loss_array, i.asnumpy()) # 其他方法的收敛值 -3.335446\n",
    "    \n",
    "res_array = np.array([])\n",
    "for i in res_list:\n",
    "    res_array = np.append(res_array, i)\n",
    "    \n",
    "print('loss_array\\n', loss_array)\n",
    "print('\\nres_array\\n', res_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e20d9",
   "metadata": {},
   "source": [
    "-4.29723059e-07  2.75024533e+00  1.13769400e+00 -6.16364069e-02 -8.08569074e-01 -5.92707038e-01 -7.09778070e-01 -2.18776774e+00 -1.48578441e+00 -1.94576359e+00 -2.08961654e+00 -2.29807091e+00 -2.99075651e+00 -2.54360437e+00 -2.45648241e+00 -3.05445695e+00 -3.18096185e+00 -2.88290739e+00 -2.82773137e+00 -2.95591903e+00 -3.04600668e+00 -3.08430839e+00 -3.21732926e+00 -2.99260306e+00 -3.05293012e+00 -3.10935092e+00 -3.15088177e+00 -3.23413754e+00 -3.21255684e+00 -3.17569113e+00 -3.18600559e+00 -3.21405339e+00 -3.28503275e+00 -3.26810479e+00 -3.24554300e+00 -3.24586272e+00 -3.28752041e+00 -3.29108167e+00 -3.28889012e+00 -3.27797341e+00 -3.30074835e+00 -3.29662442e+00 -3.29751730e+00 -3.31010938e+00 -3.29462767e+00 -3.31268477e+00 -3.31434965e+00 -3.31716490e+00 -3.31385922e+00 -3.32476568e+00 -3.31571841e+00 -3.32363844e+00 -3.32713699e+00 -3.32531166e+00 -3.32381821e+00 -3.32675385e+00 -3.33123255e+00 -3.32730246e+00 -3.32896423e+00 -3.32681131e+00 -3.33236289e+00 -3.33292770e+00 -3.33019233e+00 -3.33160925e+00 -3.33173418e+00 -3.33491611e+00 -3.33343911e+00 -3.33212233e+00 -3.33451581e+00 -3.33464813e+00 -3.33438492e+00 -3.33447790e+00 -3.33415008e+00 -3.33509231e+00 -3.33526206e+00 -3.33465242e+00 -3.33535171e+00 -3.33518028e+00 -3.33552623e+00 -3.33577657e+00 -3.33546686e+00 -3.33564258e+00 -3.33590436e+00 -3.33591604e+00 -3.33592796e+00 -3.33576775e+00 -3.33586550e+00 -3.33627081e+00 -3.33594012e+00 -3.33594370e+00 -3.33616781e+00 -3.33615875e+00 -3.33612537e+00 -3.33619666e+00 -3.33616686e+00 -3.33626604e+00 -3.33625960e+00 -3.33622479e+00 -3.33622622e+00 -3.33631635e+00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3de8d",
   "metadata": {},
   "source": [
    "0. 4. 5. 5. 5. 5. 8. 6. 5. 7. 6. 9. 6. 8. 9. 9. 8. 8. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fc46fb",
   "metadata": {},
   "source": [
    "讨论噪声影响 J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32928e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前dj为：-0.001, 损失函数值为：-3.2733586\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0009, 损失函数值为：-3.32582\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0008, 损失函数值为：-3.3152997\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0007, 损失函数值为：-3.3224034\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0006, 损失函数值为：-3.3172963\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0005, 损失函数值为：-3.1632597\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0004, 损失函数值为：-3.3035333\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0003, 损失函数值为：-3.3239388\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0002, 损失函数值为：-3.304181\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：-0.0001, 损失函数值为：-3.321882\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0, 损失函数值为：-3.320551\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0001, 损失函数值为：-3.3043017\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0002, 损失函数值为：-3.323684\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0003, 损失函数值为：-3.3159447\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0004, 损失函数值为：-3.3225322\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0005, 损失函数值为：-3.3195026\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0006, 损失函数值为：-3.3179746\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0007, 损失函数值为：-3.2920535\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0008, 损失函数值为：-3.3138962\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.0009, 损失函数值为：-3.3210092\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dj为：0.001, 损失函数值为：-3.3245845\n",
      "MBE算法计算的最大割数为： 9\n",
      "loss_list: [-3.2733585834503174, -3.325819969177246, -3.3152997493743896, -3.3224034309387207, -3.317296266555786, -3.163259744644165, -3.3035333156585693, -3.3239388465881348, -3.3041810989379883, -3.3218820095062256, -3.3205509185791016, -3.3043017387390137, -3.323683977127075, -3.3159446716308594, -3.3225321769714355, -3.319502592086792, -3.31797456741333, -3.292053461074829, -3.3138961791992188, -3.3210091590881348, -3.324584484100342]\n",
      "res_list: [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from numpy import kron\n",
    "from mindquantum import *\n",
    "from scipy.linalg import expm\n",
    "from mindspore.ops import operations\n",
    "from mindspore import nn, ops, Tensor, context\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.common.initializer import initializer  \n",
    "from mindspore.nn import Adam, TrainOneStepCell, LossBase\n",
    "ms.context.set_context(mode=ms.context.PYNATIVE_MODE, device_target=\"CPU\")\n",
    "ms.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "s_x = X.matrix()\n",
    "s_z = Z.matrix()\n",
    "one = I.matrix()\n",
    "dt = np.pi/2\n",
    "ddt = np.pi/10\n",
    "\n",
    "loss_list = []\n",
    "res_list = []\n",
    "cz_params = np.array([1.5469222,  1.4163866,  1.5392585,  1.972247,   1.92602,    1.3877034,\n",
    "                        0.8116179,  0.7628339,  1.2693673,  1.8558168,  1.0307236,  1.2505771,\n",
    "                        0.7876434,  0.30035374, 0.30896303, 0.30590588, 0.95258,    1.3812971,\n",
    "                        1.2727345,  0.56576526, 0.94235504, 0.9723476,  1.9667805,  0.8354815,\n",
    "                        1.0185238,  0.8981452,  0.2593307,  0.16673481, 0.22587535, 1.1638049,\n",
    "                        1.5322605,  0.4955098,  0.01972879, 0.02794218, 0.64640003, 0.3103799,\n",
    "                        0.4482256,  0.8699769,  1.3611945,  1.5033041,  1.1262282,  0.59920496,\n",
    "                        0.32729074, 0.54994607]) # error = 1.3743*10-6\n",
    "    # params_name = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '0', '1', '2', '3', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119']\n",
    "\n",
    "\n",
    "    \n",
    "for dj in [-0.001, -0.0009, -0.0008, -0.0007, -0.0006, -0.0005, -0.0004, -0.0003, -0.0002, -0.0001, 0, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001]:\n",
    "    def _matrix_(coeff):\n",
    "        return expm(-1j*((1+dj)*coeff*s_z+s_x)*dt)\n",
    "\n",
    "    def _diff_matrix_(coeff):\n",
    "        return -1j*_matrix_((1+dj)*coeff)@(s_z*dt)\n",
    "\n",
    "    def _matrix_0(coeff):\n",
    "        return expm(-1j*((1+dj)*coeff*s_z+s_x)*ddt)\n",
    "\n",
    "    def _diff_matrix_0(coeff):\n",
    "        return -1j*_matrix_0((1+dj)*coeff)@(s_z*ddt)\n",
    "\n",
    "    def _matrix_c_0(coeff):\n",
    "        return expm(-1j*((1+dj)*coeff*kron(s_z, one) + kron(one, s_z) + kron(s_x, one) + kron(one, s_x) + (1+dj)*coeff*kron(s_z-one, s_z-one))*5*ddt)\n",
    "\n",
    "    def _diff_matrix_c_0(coeff):\n",
    "        return -1j*_matrix_c_0((1+dj)*coeff)@((kron(s_z, one) + kron(s_z-one, s_z-one)) * 5*ddt)\n",
    "\n",
    "    def _matrix_c_1(coeff):\n",
    "        return expm(-1j*(kron(s_z, one) + (1+dj)*coeff*kron(one, s_z) + kron(s_x, one) + kron(one, s_x) + (1+dj)*coeff*kron(s_z-one, s_z-one))*5*ddt)\n",
    "\n",
    "    def _diff_matrix_c_1(coeff):\n",
    "        return -1j*_matrix_c_1((1+dj)*coeff)@((kron(one, s_z) + kron(s_z-one, s_z-one)) *  5*ddt)\n",
    "\n",
    "    gate = gene_univ_parameterized_gate('gete', _matrix_, _diff_matrix_) # dt=pi/2\n",
    "    gate_0 = gene_univ_parameterized_gate('gete_0', _matrix_0, _diff_matrix_0) # ddt=pi/10\n",
    "    gate_c_0 = gene_univ_parameterized_gate('gete_c_0', _matrix_c_0, _diff_matrix_c_0) # ddt=pi/10\n",
    "    gate_c_1 = gene_univ_parameterized_gate('gete_c_1', _matrix_c_1, _diff_matrix_c_1) # ddt=pi/10\n",
    "    \n",
    "    def cz_circ(obj_qubit, ctrl_qubit):\n",
    "        circ_ = Circuit()\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate_0(param).on(ctrl_qubit) for param in cz_params[:10]])\n",
    "        circ_ += Circuit([gate_0(0).on(obj_qubit) for i in range(10)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate_0(0).on(ctrl_qubit) for i in range(10)])\n",
    "        circ_ += Circuit([gate_0(param).on(obj_qubit) for param in cz_params[10:20]])\n",
    "        circ_ += Circuit([gate_c_0(cz_params[20]).on([obj_qubit,ctrl_qubit]), gate_c_0(cz_params[21]).on([obj_qubit,ctrl_qubit])])\n",
    "        circ_ += Circuit([gate_c_1(cz_params[22]).on([obj_qubit,ctrl_qubit]), gate_c_1(cz_params[23]).on([obj_qubit,ctrl_qubit])])\n",
    "        circ_ += Circuit([gate_0(param).on(ctrl_qubit) for param in cz_params[24:34]])\n",
    "        circ_ += Circuit([gate_0(0).on(obj_qubit) for i in range(10,20)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate_0(0).on(ctrl_qubit) for i in range(10)])\n",
    "        circ_ += Circuit([gate_0(param).on(obj_qubit) for param in cz_params[34:]])\n",
    "        circ_ += BarrierGate()\n",
    "        return circ_\n",
    "\n",
    "    def arb_circ(prefix='0'):\n",
    "        circ_ = Circuit()\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate(f'arb0{i}').on(0) for i in range(12)])\n",
    "        circ_ += Circuit([gate(0).on(1) for i in range(12)])\n",
    "        circ_ += Circuit([gate(0).on(2) for i in range(12)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate(0).on(0) for i in range(12)])\n",
    "        circ_ += Circuit([gate(f'arb1{i}').on(1) for i in range(12)])\n",
    "        circ_ += Circuit([gate(0).on(2) for i in range(12)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate(0).on(0) for i in range(12)])\n",
    "        circ_ += Circuit([gate(0).on(1) for i in range(12)])\n",
    "        circ_ += Circuit([gate(f'arb2{i}').on(2) for i in range(12)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ = add_prefix(circ_, prefix)\n",
    "        return circ_\n",
    "\n",
    "    def qlayer(prefix='0'):\n",
    "        circ_ = Circuit()\n",
    "        circ_ += arb_circ('0'+prefix)\n",
    "        circ_ += cz_circ(1, 0)\n",
    "        circ_ += Circuit([gate_0(0).on(2) for i in range(10)])\n",
    "        circ_ += cz_circ(2, 1)\n",
    "        circ_ += Circuit([gate_0(0).on(0) for i in range(10)])\n",
    "        return circ_\n",
    "\n",
    "    class my_simulator: \n",
    "        def get_expectation_with_grad(self, hams, circ): # 输入为 hams, circ_right, simulator_left 等，这里就只用 hams 代替。\n",
    "            sim = Simulator('projectq', 3) \n",
    "            \n",
    "            def grad_ops(inputs): # 输入为各量子门的参数\n",
    "                h = 1e-4\n",
    "                grad = []\n",
    "                sim.reset()\n",
    "                sim.apply_circuit(circ, inputs)\n",
    "                exceptation = [sim.get_expectation(ham).real for ham in hams]\n",
    "                for i in range(inputs.size):\n",
    "                    params_p, params_n = copy.deepcopy(inputs), copy.deepcopy(inputs)\n",
    "                    params_p[i] += h\n",
    "                    params_n[i] -= h\n",
    "                    sim.reset()\n",
    "                    sim.apply_circuit(circ, params_p)\n",
    "                    g_p = [sim.get_expectation(ham) for ham in hams]\n",
    "                    sim.reset()\n",
    "                    sim.apply_circuit(circ, params_n)\n",
    "                    g_n = [sim.get_expectation(ham) for ham in hams]\n",
    "                    grad.append([(p.real - n.real)/(2*h) for p, n in zip(g_p, g_n)])\n",
    "                    sim.reset()\n",
    "                return exceptation, np.array(grad).T # 运行结果为期望值和梯度\n",
    "            return grad_ops # MindQuantum 中，这里返回的是一个封装了 hams, grad_ops 等的一个封装器，这里简化为单独的 grad_ops\n",
    "        \n",
    "    hams = [Hamiltonian(QubitOperator('Z0')), Hamiltonian(QubitOperator('X0')), \n",
    "            Hamiltonian(QubitOperator('Z1')), Hamiltonian(QubitOperator('X1')),\n",
    "            Hamiltonian(QubitOperator('Z2')), Hamiltonian(QubitOperator('X2'))]\n",
    "\n",
    "    class ansatz_only_ops(nn.Cell):\n",
    "        def __init__(self, expectation_with_grad):\n",
    "            super().__init__()\n",
    "            self.expectation_with_grad = expectation_with_grad\n",
    "            self.shape_ops = operations.Shape()\n",
    "            self.g = None \n",
    "\n",
    "        def construct(self, arg):\n",
    "            fval, g_ans = self.expectation_with_grad(arg.asnumpy())\n",
    "            self.g = np.real(g_ans)\n",
    "            return ms.Tensor(np.real(fval), dtype=ms.float32)\n",
    "\n",
    "        def bprop(self, arg, out, dout): \n",
    "            dout = dout.asnumpy()\n",
    "            grad = dout @ self.g\n",
    "            return ms.Tensor(grad, dtype=ms.float32)\n",
    "\n",
    "    class ansatz_only_layer(nn.Cell):\n",
    "        def __init__(self, expectation_with_grad, weight='ones'):\n",
    "            super().__init__()\n",
    "            self.evolution = ansatz_only_ops(expectation_with_grad)\n",
    "            weight_size = len(circ.params_name)\n",
    "            self.weight = Parameter(initializer(weight, weight_size, dtype=ms.float32), name='ansatz_weight')\n",
    "            self.abs = ops.Abs()\n",
    "\n",
    "        def construct(self):\n",
    "            return self.evolution(self.abs(self.weight))\n",
    "\n",
    "    class MyLoss(LossBase):\n",
    "        def __init__(self, reduction='mean'):\n",
    "            super(MyLoss, self).__init__(reduction)\n",
    "            self.tanh = ops.Tanh()\n",
    "\n",
    "        def construct(self, logits):\n",
    "            x = self.tanh(logits)\n",
    "            out = 0\n",
    "            for edge in edges:\n",
    "                out += x[edge[0]] * x[edge[1]]\n",
    "            return self.get_loss(out)\n",
    "\n",
    "    class MyWithLossCell(nn.Cell):\n",
    "        def __init__(self, backbone, loss_fn):\n",
    "            super(MyWithLossCell, self).__init__(auto_prefix=False)\n",
    "            self._backbone = backbone\n",
    "            self._loss_fn = loss_fn\n",
    "\n",
    "        def construct(self):\n",
    "            out = self._backbone()\n",
    "            return self._loss_fn(out)\n",
    "\n",
    "        @property\n",
    "        def backbone_network(self):\n",
    "            return self._backbone\n",
    "        \n",
    "    edges = []\n",
    "    node_num = 6\n",
    "    for node in range(node_num-1): \n",
    "        edges.append([node, node+1])\n",
    "    edges.append([node_num-1, 0])\n",
    "    for node in range(int(node_num/2)):\n",
    "        edges.append([node, int(node+node_num/2)])\n",
    "    circ = Circuit()\n",
    "    for i in range(2):\n",
    "        circ += qlayer(prefix=f'{i}')\n",
    "    my_sim = my_simulator()\n",
    "    grad_ops = my_sim.get_expectation_with_grad(hams, circ)\n",
    "    qnet = ansatz_only_layer(grad_ops)\n",
    "    loss = MyLoss()\n",
    "    net_with_criterion = MyWithLossCell(qnet, loss)\n",
    "    opti = Adam(qnet.trainable_params(), learning_rate=0.1) \n",
    "    net = TrainOneStepCell(net_with_criterion, opti)\n",
    "\n",
    "    round = ops.Round()\n",
    "    for i in range(51):\n",
    "        loss = net()\n",
    "    loss_list.append(float(loss.asnumpy()))\n",
    "    print(f'\\n当前dj为：{dj}, 损失函数值为：{loss}')\n",
    "    out = qnet()\n",
    "    result = 0\n",
    "    for edge in edges:\n",
    "        result += (1 - round(out[edge[0]]) * round(out[edge[1]])) / 2\n",
    "    print('MBE算法计算的最大割数为：', int(result.asnumpy() + 0.5))\n",
    "    res_list.append(int(result.asnumpy() + 0.5))\n",
    "print('loss_list:', loss_list)\n",
    "print('res_list:', res_list)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dba38c",
   "metadata": {},
   "source": [
    "讨论噪声影响 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a881b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前dh为：0.0002, 损失函数值为：-3.324049\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dh为：0.0003, 损失函数值为：-3.3151553\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dh为：0.0004, 损失函数值为：-3.3133185\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dh为：0.0005, 损失函数值为：-3.3088465\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dh为：0.0006, 损失函数值为：-3.3120005\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dh为：0.0007, 损失函数值为：-3.3148396\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dh为：0.0008, 损失函数值为：-3.3226714\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dh为：0.0009, 损失函数值为：-3.3040495\n",
      "MBE算法计算的最大割数为： 9\n",
      "\n",
      "当前dh为：0.001, 损失函数值为：-3.3175426\n",
      "MBE算法计算的最大割数为： 9\n",
      "loss_list: [-3.3240489959716797, -3.315155267715454, -3.3133184909820557, -3.3088464736938477, -3.3120005130767822, -3.3148396015167236, -3.322671413421631, -3.304049491882324, -3.317542552947998]\n",
      "res_list: [9, 9, 9, 9, 9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from numpy import kron\n",
    "from mindquantum import *\n",
    "from scipy.linalg import expm\n",
    "from mindspore.ops import operations\n",
    "from mindspore import nn, ops, Tensor, context\n",
    "from mindspore.common.parameter import Parameter\n",
    "from mindspore.common.initializer import initializer  \n",
    "from mindspore.nn import Adam, TrainOneStepCell, LossBase\n",
    "ms.context.set_context(mode=ms.context.PYNATIVE_MODE, device_target=\"CPU\")\n",
    "ms.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "s_x = X.matrix()\n",
    "s_z = Z.matrix()\n",
    "one = I.matrix()\n",
    "dt = np.pi/2\n",
    "ddt = np.pi/10\n",
    "\n",
    "loss_list = []\n",
    "res_list = []\n",
    "cz_params = np.array([1.5469222,  1.4163866,  1.5392585,  1.972247,   1.92602,    1.3877034,\n",
    "                        0.8116179,  0.7628339,  1.2693673,  1.8558168,  1.0307236,  1.2505771,\n",
    "                        0.7876434,  0.30035374, 0.30896303, 0.30590588, 0.95258,    1.3812971,\n",
    "                        1.2727345,  0.56576526, 0.94235504, 0.9723476,  1.9667805,  0.8354815,\n",
    "                        1.0185238,  0.8981452,  0.2593307,  0.16673481, 0.22587535, 1.1638049,\n",
    "                        1.5322605,  0.4955098,  0.01972879, 0.02794218, 0.64640003, 0.3103799,\n",
    "                        0.4482256,  0.8699769,  1.3611945,  1.5033041,  1.1262282,  0.59920496,\n",
    "                        0.32729074, 0.54994607]) # error = 1.3743*10-6\n",
    "    # params_name = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '0', '1', '2', '3', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119']\n",
    "\n",
    "\n",
    "    \n",
    "for dh in[0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001]: # -0.001, -0.0009, -0.0008, -0.0007, -0.0006, -0.0005, -0.0004, -0.0003, -0.0002, -0.0001, 0, 0.0001, \n",
    "    def _matrix_(coeff):\n",
    "        return expm(-1j*(coeff*s_z+(1+dh)*s_x)*dt)\n",
    "\n",
    "    def _diff_matrix_(coeff):\n",
    "        return -1j*_matrix_(coeff)@(s_z*dt)\n",
    "\n",
    "    def _matrix_0(coeff):\n",
    "        return expm(-1j*(coeff*s_z+(1+dh)*s_x)*ddt)\n",
    "\n",
    "    def _diff_matrix_0(coeff):\n",
    "        return -1j*_matrix_0(coeff)@(s_z*ddt)\n",
    "\n",
    "    def _matrix_c_0(coeff):\n",
    "        return expm(-1j*(coeff*kron(s_z, one) + kron(one, s_z) + kron((1+dh)*s_x, one) + kron(one, (1+dh)*s_x) + coeff*kron(s_z-one, s_z-one))*5*ddt)\n",
    "\n",
    "    def _diff_matrix_c_0(coeff):\n",
    "        return -1j*_matrix_c_0(coeff)@((kron(s_z, one) + kron(s_z-one, s_z-one)) * 5*ddt)\n",
    "\n",
    "    def _matrix_c_1(coeff):\n",
    "        return expm(-1j*(kron(s_z, one) + coeff*kron(one, s_z) + kron((1+dh)*s_x, one) + kron(one, (1+dh)*s_x) + coeff*kron(s_z-one, s_z-one))*5*ddt)\n",
    "\n",
    "    def _diff_matrix_c_1(coeff):\n",
    "        return -1j*_matrix_c_1(coeff)@((kron(one, s_z) + kron(s_z-one, s_z-one)) *  5*ddt)\n",
    "\n",
    "    gate = gene_univ_parameterized_gate('gete', _matrix_, _diff_matrix_) # dt=pi/2\n",
    "    gate_0 = gene_univ_parameterized_gate('gete_0', _matrix_0, _diff_matrix_0) # ddt=pi/10\n",
    "    gate_c_0 = gene_univ_parameterized_gate('gete_c_0', _matrix_c_0, _diff_matrix_c_0) # ddt=pi/10\n",
    "    gate_c_1 = gene_univ_parameterized_gate('gete_c_1', _matrix_c_1, _diff_matrix_c_1) # ddt=pi/10\n",
    "    \n",
    "    def cz_circ(obj_qubit, ctrl_qubit):\n",
    "        circ_ = Circuit()\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate_0(param).on(ctrl_qubit) for param in cz_params[:10]])\n",
    "        circ_ += Circuit([gate_0(0).on(obj_qubit) for i in range(10)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate_0(0).on(ctrl_qubit) for i in range(10)])\n",
    "        circ_ += Circuit([gate_0(param).on(obj_qubit) for param in cz_params[10:20]])\n",
    "        circ_ += Circuit([gate_c_0(cz_params[20]).on([obj_qubit,ctrl_qubit]), gate_c_0(cz_params[21]).on([obj_qubit,ctrl_qubit])])\n",
    "        circ_ += Circuit([gate_c_1(cz_params[22]).on([obj_qubit,ctrl_qubit]), gate_c_1(cz_params[23]).on([obj_qubit,ctrl_qubit])])\n",
    "        circ_ += Circuit([gate_0(param).on(ctrl_qubit) for param in cz_params[24:34]])\n",
    "        circ_ += Circuit([gate_0(0).on(obj_qubit) for i in range(10,20)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate_0(0).on(ctrl_qubit) for i in range(10)])\n",
    "        circ_ += Circuit([gate_0(param).on(obj_qubit) for param in cz_params[34:]])\n",
    "        circ_ += BarrierGate()\n",
    "        return circ_\n",
    "\n",
    "    def arb_circ(prefix='0'):\n",
    "        circ_ = Circuit()\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate(f'arb0{i}').on(0) for i in range(12)])\n",
    "        circ_ += Circuit([gate(0).on(1) for i in range(12)])\n",
    "        circ_ += Circuit([gate(0).on(2) for i in range(12)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate(0).on(0) for i in range(12)])\n",
    "        circ_ += Circuit([gate(f'arb1{i}').on(1) for i in range(12)])\n",
    "        circ_ += Circuit([gate(0).on(2) for i in range(12)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ += Circuit([gate(0).on(0) for i in range(12)])\n",
    "        circ_ += Circuit([gate(0).on(1) for i in range(12)])\n",
    "        circ_ += Circuit([gate(f'arb2{i}').on(2) for i in range(12)])\n",
    "        circ_ += BarrierGate()\n",
    "        circ_ = add_prefix(circ_, prefix)\n",
    "        return circ_\n",
    "\n",
    "    def qlayer(prefix='0'):\n",
    "        circ_ = Circuit()\n",
    "        circ_ += arb_circ('0'+prefix)\n",
    "        circ_ += cz_circ(1, 0)\n",
    "        circ_ += Circuit([gate_0(0).on(2) for i in range(10)])\n",
    "        circ_ += cz_circ(2, 1)\n",
    "        circ_ += Circuit([gate_0(0).on(0) for i in range(10)])\n",
    "        return circ_\n",
    "\n",
    "    class my_simulator: \n",
    "        def get_expectation_with_grad(self, hams, circ): # 输入为 hams, circ_right, simulator_left 等，这里就只用 hams 代替。\n",
    "            sim = Simulator('projectq', 3) \n",
    "            \n",
    "            def grad_ops(inputs): # 输入为各量子门的参数\n",
    "                h = 1e-4\n",
    "                grad = []\n",
    "                sim.reset()\n",
    "                sim.apply_circuit(circ, inputs)\n",
    "                exceptation = [sim.get_expectation(ham).real for ham in hams]\n",
    "                for i in range(inputs.size):\n",
    "                    params_p, params_n = copy.deepcopy(inputs), copy.deepcopy(inputs)\n",
    "                    params_p[i] += h\n",
    "                    params_n[i] -= h\n",
    "                    sim.reset()\n",
    "                    sim.apply_circuit(circ, params_p)\n",
    "                    g_p = [sim.get_expectation(ham) for ham in hams]\n",
    "                    sim.reset()\n",
    "                    sim.apply_circuit(circ, params_n)\n",
    "                    g_n = [sim.get_expectation(ham) for ham in hams]\n",
    "                    grad.append([(p.real - n.real)/(2*h) for p, n in zip(g_p, g_n)])\n",
    "                    sim.reset()\n",
    "                return exceptation, np.array(grad).T # 运行结果为期望值和梯度\n",
    "            return grad_ops # MindQuantum 中，这里返回的是一个封装了 hams, grad_ops 等的一个封装器，这里简化为单独的 grad_ops\n",
    "        \n",
    "    hams = [Hamiltonian(QubitOperator('Z0')), Hamiltonian(QubitOperator('X0')), \n",
    "            Hamiltonian(QubitOperator('Z1')), Hamiltonian(QubitOperator('X1')),\n",
    "            Hamiltonian(QubitOperator('Z2')), Hamiltonian(QubitOperator('X2'))]\n",
    "\n",
    "    class ansatz_only_ops(nn.Cell):\n",
    "        def __init__(self, expectation_with_grad):\n",
    "            super().__init__()\n",
    "            self.expectation_with_grad = expectation_with_grad\n",
    "            self.shape_ops = operations.Shape()\n",
    "            self.g = None \n",
    "\n",
    "        def construct(self, arg):\n",
    "            fval, g_ans = self.expectation_with_grad(arg.asnumpy())\n",
    "            self.g = np.real(g_ans)\n",
    "            return ms.Tensor(np.real(fval), dtype=ms.float32)\n",
    "\n",
    "        def bprop(self, arg, out, dout): \n",
    "            dout = dout.asnumpy()\n",
    "            grad = dout @ self.g\n",
    "            return ms.Tensor(grad, dtype=ms.float32)\n",
    "\n",
    "    class ansatz_only_layer(nn.Cell):\n",
    "        def __init__(self, expectation_with_grad, weight='ones'):\n",
    "            super().__init__()\n",
    "            self.evolution = ansatz_only_ops(expectation_with_grad)\n",
    "            weight_size = len(circ.params_name)\n",
    "            self.weight = Parameter(initializer(weight, weight_size, dtype=ms.float32), name='ansatz_weight')\n",
    "            self.abs = ops.Abs()\n",
    "\n",
    "        def construct(self):\n",
    "            return self.evolution(self.abs(self.weight))\n",
    "\n",
    "    class MyLoss(LossBase):\n",
    "        def __init__(self, reduction='mean'):\n",
    "            super(MyLoss, self).__init__(reduction)\n",
    "            self.tanh = ops.Tanh()\n",
    "\n",
    "        def construct(self, logits):\n",
    "            x = self.tanh(logits)\n",
    "            out = 0\n",
    "            for edge in edges:\n",
    "                out += x[edge[0]] * x[edge[1]]\n",
    "            return self.get_loss(out)\n",
    "\n",
    "    class MyWithLossCell(nn.Cell):\n",
    "        def __init__(self, backbone, loss_fn):\n",
    "            super(MyWithLossCell, self).__init__(auto_prefix=False)\n",
    "            self._backbone = backbone\n",
    "            self._loss_fn = loss_fn\n",
    "\n",
    "        def construct(self):\n",
    "            out = self._backbone()\n",
    "            return self._loss_fn(out)\n",
    "\n",
    "        @property\n",
    "        def backbone_network(self):\n",
    "            return self._backbone\n",
    "        \n",
    "    edges = []\n",
    "    node_num = 6\n",
    "    for node in range(node_num-1): \n",
    "        edges.append([node, node+1])\n",
    "    edges.append([node_num-1, 0])\n",
    "    for node in range(int(node_num/2)):\n",
    "        edges.append([node, int(node+node_num/2)])\n",
    "    circ = Circuit()\n",
    "    for i in range(2):\n",
    "        circ += qlayer(prefix=f'{i}')\n",
    "    my_sim = my_simulator()\n",
    "    grad_ops = my_sim.get_expectation_with_grad(hams, circ)\n",
    "    qnet = ansatz_only_layer(grad_ops)\n",
    "    loss = MyLoss()\n",
    "    net_with_criterion = MyWithLossCell(qnet, loss)\n",
    "    opti = Adam(qnet.trainable_params(), learning_rate=0.1) \n",
    "    net = TrainOneStepCell(net_with_criterion, opti)\n",
    "\n",
    "    round = ops.Round()\n",
    "    for i in range(51):\n",
    "        loss = net()\n",
    "    loss_list.append(float(loss.asnumpy()))\n",
    "    print(f'\\n当前dh为：{dh}, 损失函数值为：{loss}')\n",
    "    out = qnet()\n",
    "    result = 0\n",
    "    for edge in edges:\n",
    "        result += (1 - round(out[edge[0]]) * round(out[edge[1]])) / 2\n",
    "    print('MBE算法计算的最大割数为：', int(result.asnumpy() + 0.5))\n",
    "    res_list.append(int(result.asnumpy() + 0.5))\n",
    "print('loss_list:', loss_list)\n",
    "print('res_list:', res_list)\n",
    "# -3.2765427 -3.0640457 -3.3120396 -3.309067 -3.314459 -3.3166137 -3.3151793 -3.2924461 -3.2425184 -3.3103433 -3.320551 -3.3103735\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "当前dh为：-0.001, 损失函数值为：-3.2765427\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0009, 损失函数值为：-3.0640457\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0008, 损失函数值为：-3.3120396\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0007, 损失函数值为：-3.309067\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0006, 损失函数值为：-3.314459\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0005, 损失函数值为：-3.3166137\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0004, 损失函数值为：-3.3151793\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0003, 损失函数值为：-3.2924461\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0002, 损失函数值为：-3.2425184\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：-0.0001, 损失函数值为：-3.3103433\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：0, 损失函数值为：-3.320551\n",
    "MBE算法计算的最大割数为： 9\n",
    "\n",
    "当前dh为：0.0001, 损失函数值为：-3.3103735\n",
    "MBE算法计算的最大割数为： 9\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
