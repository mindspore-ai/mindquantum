View demo code of this section: \democode{05}{5.4.6}
\textit{Background} -- Singular Value Decomposition (SVD) is an important matrix decomposition in linear algebra. As a generalization of feature decomposition on arbitrary dimensional matrices, SVD is widely used in the field of machine learning, including matrix compression, recommendation systems, and natural language processing. It is defined as follows:
Given a complex matrix $M \in \mathbb{C}^{m \times n}$, define the SVD of the matrix $M$ as $M = UDV^\dagger$. Where $U$ is an $m \times m$ matrix, $V$ is an $n \times n$ matrix, and both $U$ and $V$ are unitary matrices, that is, satisfy $UU^\dagger = I$, $VV^\dagger = I$. $D$ is a diagonal matrix $m \times n$, with the elements of the main diagonal arranged from largest to smallest, and each element is called a singular value of the matrix $M$.


Quantum algorithms for SVD have been proposed in \cite{kerenidis2016quantum, rebentrost2018quantum}, which leads to applications in solving linear systems of equations \cite{wossnig2018quantum} and developing quantum recommendation systems \cite{kerenidis2016quantum}. However, these algorithms above are too costly to be convincingly validated for near-term quantum devices. The leading strategy to solve various problems using noisy intermediate-scale quantum (NISQ) devices are called variational quantum algorithms.

\cite{wang2021variational} propose a variational quantum algorithm for singular value decomposition (VQSVD), formulating the task of SVD as an optimization problem. The detailed VQSVD algorithm is as follows:
\begin{enumerate}
    \item Prepare the input of VQSVD algorithm:
        \begin{itemize}
            \item A decomposition of the matrix $M$ into a linear combination of $K$ unitaries of the form $M = \sum_{k=1}^K c_k A_k$ with real numbers $c_k$.
            \item Positive weights $q_1 > \cdots > q_r > 0$.
            \item Computational basis {$\ket{\psi_1}, \cdots, \ket{\psi_T}$}, where $T$ is the desired rank.
            \item Parameterized circuits $U(\theta)$, $V(\phi)$ with initial parameters of $\theta, \phi$.
        \end{itemize}
    \item Enter a hybrid quantum-classical optimization loop to train the parameters $\theta$ and $\phi$ in the parameterized quantum circuits $U(\theta)$ and $V(\phi)$, compute the singular values of $M$: $m_j = \text{Re}\langle\psi_j|U(\theta)^{\dagger} M V(\phi)\ket{\psi_j}$. The goal is to maximum the designed loss $L(\theta, \phi)$:
    \begin{equation}
        L(\theta, \phi) = \sum_{j=1}^T q_j \times \text{Re} \langle\psi_j| U(\theta)^{\dagger} M V(\phi)\ket{\psi_j}
    \end{equation}
    Where weights are added to make the calculated singular values descending.
    \item Obtain optimal parameters $\alpha^ \star$ and $\beta^\star$ and compute $U(\alpha^\star)$ and $V(\beta^\star)$.
    \item Obtain approximate singular values ${m_1, \cdots, m_r}$ and  singular vectors $U(\alpha^\star)$ and $V(\beta^\star)$.
\end{enumerate}

\textit{Application} -- In this section, we show how to decompose a randomly generated $8 \times 8$ complex matrix using MindQuantum.

First we need to introduce the required packages, define the required constants and set the weights, then use the $numpy.random.randint$ function to randomly generate an $8 \times 8$ complex matrix $M$:
\begin{lstlisting}
from mindquantum import Simulator, MQAnsatzOnlyLayer, add_prefix
from mindquantum import Hamiltonian, Circuit, RY, RZ, X
import mindspore as ms
import numpy as np
from scipy.sparse import csr_matrix
from scipy.linalg import norm
from matplotlib import pyplot
import tqdm

n_qubits = 3  # qbits number
cir_depth = 20  # circuit depth
N = 2**n_qubits
rank = 8  # learning rank
step = 3
ITR = 200  # iterations
LR = 0.02  # learning rate

# Set equal learning weights
if step == 0:
    weight = ms.Tensor(np.ones(rank))
else:
    weight = ms.Tensor(np.arange(rank * step, 0, -step))

# Define random seed
np.random.seed(42)

def mat_generator():
    '''
    Generate a random complex matrix
    '''
    matrix = np.random.randint(
        10, size=(N, N)) + 1j * np.random.randint(10, size=(N, N))
    return matrix

# Generate matrix M which will be decomposed
M = mat_generator()
# m_copy is generated to error analysis
m_copy = np.copy(M)
print('Random matrix M is: ')
print(M)
# Get SVD results
U, D, v_dagger = np.linalg.svd(M, full_matrices=True)
\end{lstlisting}
Next define a hardware-efficient ansatz used in the simulation for $U(\theta)$ and $V(\phi)$, the variational ansatz used in the paper \cite{wang2021variational}. Since only real matrices are involved, the combination of rotation gates and CNOT is sufficient. In the ansatz, each same block(denoted in the dashed-line box) consists of a column of single-qubit rotations about the y-axis and z-axis following by a layer of CNOT gates which only connects the adjacent qubits.
\begin{lstlisting}
class Ansatz:
    def __init__(self, n, depth):
        self.circ = Circuit()
        num = 0
        for _ in range(depth):
            for i in range(n):
                self.circ += RY('theta' + str(num)).on(i)
                num += 1
            for i in range(n):
                self.circ += RZ('theta' + str(num)).on(i)
                num += 1
            for i in range(n - 1):
                self.circ += X.on(i + 1, i)
            self.circ += X.on(0, n - 1)
\end{lstlisting}
Then define a quantum network using the given hamiltonian. The $set_q$ function of the simulator is used to set the state of the simulator to the given computational basis, so that we can obtain the measurement results under the basis, that is, the corresponding singular values. The $get\_expectation\_with\_grad$ function is used to compute the gradient of the parameters in the circuit and the value of the following expression: $E(\theta) = \langle\phi| U_l^{\dagger}(\theta) H U_r(\theta)\ket{\psi}$. Then we can use $MQAnsatzOnlyLayer$ to build a quantum network layer based on a given basis, and its output is: $\text{Re}\langle\psi_j|U(\theta)^{\dagger} M V(\phi)\ket{\psi_j}$.
\begin{lstlisting}
def quantnet(qubits_num, hams, circ_right, circ_left=None, base=None):
    sim = Simulator('mqvector', qubits_num)
    if base is None:
        pass
    else:
        sim.set_qs(base)
    grad_ops = sim.get_expectation_with_grad(hams, circ_right, circ_left)

    quantumnet = MQAnsatzOnlyLayer(grad_ops, 'ones')
    return quantumnet
\end{lstlisting}
After sparring the decomposed $8 \times 8$ matrix $M$ and generating the corresponding Hamiltonian $H$, we can then instantiate ansatz $U_ansatz$ and $V_ansatz$ and build a required quantum network layer, which can be used to compute $\{m_j\}_{j=1}^T$, the singular values of $M$.
\begin{lstlisting}
u_ansatz = add_prefix(Ansatz(n_qubits, cir_depth).circ, 'u')
v_ansatz = add_prefix(Ansatz(n_qubits, cir_depth).circ, 'v')
ham = Hamiltonian(csr_matrix(M))
i_matrix = np.identity(N)
quantum_models = dict()
quantum_models['net_0'] = quantnet(n_qubits, ham, v_ansatz, u_ansatz, i_matrix[0])
for s in range(1, rank):
    quantum_models["net_" + str(s)] = quantnet(n_qubits, ham, v_ansatz, u_ansatz, i_matrix[s])
    quantum_models["net_" + str(s)].weight = quantum_models['net_0'].weight
\end{lstlisting}
Additionally, we can use MindSpore to build a hybrid quantum-classical network to realize weighted summation of quantum network layers and compute $L(\theta,\phi) = \sum_{j=1}^T q_j \times \text{Re} \langle\psi_j| U(\theta)^{\dagger} M V(\phi)\ket{\psi_j}$.
\begin{lstlisting}
class MyNet(ms.nn.Cell):
    '''
    define quantum-classic net
    '''
    def __init__(self):
        super(MyNet, self).__init__()

        self.build_block = ms.nn.CellList()
        for j in range(rank):
            self.build_block.append(quantum_models["net_" + str(j)])

    def construct(self):
        x = self.build_block[0]() * weight[0]
        k = 1
        for layer in self.build_block[1:]:
            x += layer() * weight[k]
            k += 1
        return -x
\end{lstlisting}
Now we can instantiate the hybrid quantum-classical network and start training using MindSpore:
\begin{lstlisting}
net = MyNet()
# Define optimizer
opt = ms.nn.Adam(net.trainable_params(), learning_rate=LR)
# Simple gradient descent
train_net = ms.nn.TrainOneStepCell(net, opt)
# Start training
loss_list = list()
for itr in tqdm.tqdm(range(ITR)):
    res = train_net()
    loss_list.append(res.asnumpy().tolist())
\end{lstlisting}
Finally read the training results (the singular values) and compare them with the results of the classical singular value decomposition:
\begin{lstlisting}
singular_value = list()
for _, qnet in quantum_models.items():
    singular_value.append(qnet().asnumpy()[0])

print('Predicted singular values from large to small:', singular_value)
print("True singular values from large to small:", D)
\end{lstlisting}
Output:
\begin{lstlisting}
Predicted singular values from large to small: [54.83174, 19.169168, 14.88653, 11.093878, 10.533753, 7.648352, 5.5560594, -0.3320913]
True singular values from large to small: [54.83484985 19.18141073 14.98866247 11.61419557 10.15927045  7.60223249 5.81040539  3.30116001]
\end{lstlisting}
Intuitively, we can see that the singular values learned by using the hybrid quantum-classical network are similar to the real singular values.
