{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "tags": [
                    "outputPrepend"
                ]
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[WARNING] ME(127525:140012191486784,MainProcess):2023-10-17-23:52:56.903.224 [mindspore/run_check/_check_version.py:102] MindSpore version 2.1.0 and cuda version 11.2.72 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n",
                        "/tmp/ipykernel_127525/920738167.py:58: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{0}').on([i, i+1])\n",
                        "/tmp/ipykernel_127525/920738167.py:59: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{0}').on([qubit_num-1, 0])\n",
                        "/tmp/ipykernel_127525/920738167.py:67: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{2}').on([i, i+1])\n",
                        "/tmp/ipykernel_127525/920738167.py:68: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{2}').on([qubit_num-1, 0])\n",
                        "/tmp/ipykernel_127525/920738167.py:76: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{4}').on([i, i+1])\n",
                        "/tmp/ipykernel_127525/920738167.py:77: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'4').on([qubit_num-1, 0])\n",
                        "/tmp/ipykernel_127525/920738167.py:85: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{6}').on([i, i+1])\n",
                        "/tmp/ipykernel_127525/920738167.py:86: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{6}').on([qubit_num-1, 0])\n",
                        "/tmp/ipykernel_127525/920738167.py:94: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{8}').on([i, i+1])\n",
                        "/tmp/ipykernel_127525/920738167.py:95: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{8}').on([qubit_num-1, 0])\n",
                        "/tmp/ipykernel_127525/920738167.py:103: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{10}').on([i, i+1])\n",
                        "/tmp/ipykernel_127525/920738167.py:104: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  encoder += ZZ(f'{10}').on([qubit_num-1, 0])\n",
                        "/tmp/ipykernel_127525/920738167.py:192: DeprecationWarning: ZZ gate is deprecated, please use Rzz\n",
                        "  _circ += ZZ('06').on([bit_up, bit_down])\n",
                        "/tmp/ipykernel_127525/920738167.py:193: DeprecationWarning: YY gate is deprecated, please use Ryy\n",
                        "  _circ += YY('07').on([bit_up, bit_down])\n",
                        "/tmp/ipykernel_127525/920738167.py:194: DeprecationWarning: XX gate is deprecated, please use Rxx\n",
                        "  _circ += XX('08').on([bit_up, bit_down])\n",
                        "[WARNING] ME(127525:140012191486784,MainProcess):2023-10-17-23:52:58.166.535 [mindspore/train/model.py:1098] For StepAcc callback, {'step_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch: 1 step: 1, loss is 0.6988517642021179\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 2, loss is 0.6907015442848206\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 3, loss is 0.7001867294311523\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 4, loss is 0.689882755279541\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 5, loss is 0.6969389319419861\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 6, loss is 0.702521562576294\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 7, loss is 0.6937159895896912\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 8, loss is 0.6992268562316895\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 9, loss is 0.6976926922798157\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 10, loss is 0.6927091479301453\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 11, loss is 0.6993816494941711\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 12, loss is 0.6906757354736328\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 13, loss is 0.6955680251121521\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 14, loss is 0.6904752254486084\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 15, loss is 0.698263943195343\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 16, loss is 0.6916778087615967\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 17, loss is 0.6892223358154297\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 18, loss is 0.692651093006134\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 19, loss is 0.6922605633735657\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 1 step: 20, loss is 0.6894490122795105\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 2 step: 1, loss is 0.6936027407646179\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 2 step: 2, loss is 0.6850547790527344\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 2 step: 3, loss is 0.6912743449211121\n",
                        "acc is: 0.5714285714285714\n",
                        "epoch: 2 step: 4, loss is 0.6841999888420105\n",
                        "acc is: 0.6190476190476191\n",
                        "epoch: 2 step: 5, loss is 0.6927667260169983\n",
                        "acc is: 0.6190476190476191\n",
                        "epoch: 2 step: 6, loss is 0.6945667862892151\n",
                        "acc is: 0.7142857142857143\n",
                        "epoch: 2 step: 7, loss is 0.6859754920005798\n",
                        "acc is: 0.7142857142857143\n",
                        "epoch: 2 step: 8, loss is 0.6893119215965271\n",
                        "acc is: 0.7142857142857143\n",
                        "epoch: 2 step: 9, loss is 0.6884511113166809\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 2 step: 10, loss is 0.6846775412559509\n",
                        "acc is: 0.7619047619047619\n",
                        "epoch: 2 step: 11, loss is 0.6929135322570801\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 2 step: 12, loss is 0.6841044425964355\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 2 step: 13, loss is 0.6864350438117981\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 2 step: 14, loss is 0.6830921173095703\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 2 step: 15, loss is 0.6949553489685059\n",
                        "acc is: 0.8095238095238095\n",
                        "epoch: 2 step: 16, loss is 0.6861870288848877\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 2 step: 17, loss is 0.681009829044342\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 2 step: 18, loss is 0.683655321598053\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 2 step: 19, loss is 0.6840494275093079\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 2 step: 20, loss is 0.6795856952667236\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 1, loss is 0.6883743405342102\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 2, loss is 0.6759818196296692\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 3, loss is 0.6833052635192871\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 4, loss is 0.6735110282897949\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 5, loss is 0.6893779635429382\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 6, loss is 0.6888582706451416\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 7, loss is 0.6759472489356995\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 8, loss is 0.6785211563110352\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 9, loss is 0.6782972812652588\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 10, loss is 0.6738300919532776\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 11, loss is 0.6875874400138855\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 12, loss is 0.6742382049560547\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 13, loss is 0.6755051612854004\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 14, loss is 0.6723649501800537\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 15, loss is 0.6917456984519958\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 16, loss is 0.6768097877502441\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 17, loss is 0.6694096922874451\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 18, loss is 0.6723600029945374\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 19, loss is 0.6730852723121643\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 3 step: 20, loss is 0.664974570274353\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 1, loss is 0.6800664067268372\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 2, loss is 0.6630527377128601\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 3, loss is 0.6726948618888855\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 4, loss is 0.6582115292549133\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 5, loss is 0.6846280097961426\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 6, loss is 0.6797571778297424\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 7, loss is 0.6617047190666199\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 8, loss is 0.6622248291969299\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 9, loss is 0.6630730628967285\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 10, loss is 0.6584271192550659\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 11, loss is 0.6793158054351807\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 12, loss is 0.660561740398407\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 13, loss is 0.6601445078849792\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 14, loss is 0.6578704714775085\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 15, loss is 0.6854448318481445\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 16, loss is 0.6640603542327881\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 17, loss is 0.6541451811790466\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 4 step: 18, loss is 0.6571164131164551\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 4 step: 19, loss is 0.6582120060920715\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 4 step: 20, loss is 0.6461337804794312\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 1, loss is 0.6685628890991211\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 2, loss is 0.6478036046028137\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 3, loss is 0.659509003162384\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 4, loss is 0.6403210163116455\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 5, loss is 0.6786362528800964\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 6, loss is 0.667915403842926\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 7, loss is 0.6452823281288147\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 8, loss is 0.6433925032615662\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 5 step: 9, loss is 0.6456794738769531\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 5 step: 10, loss is 0.6413803696632385\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 5 step: 11, loss is 0.669741153717041\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 5 step: 12, loss is 0.6456625461578369\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 5 step: 13, loss is 0.6439453959465027\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 14, loss is 0.6425842642784119\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 15, loss is 0.6791337132453918\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 16, loss is 0.6515620946884155\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 17, loss is 0.6384691596031189\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 18, loss is 0.6419695615768433\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 19, loss is 0.6438273191452026\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 5 step: 20, loss is 0.6280632019042969\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 1, loss is 0.6585845947265625\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 2, loss is 0.632207453250885\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 3, loss is 0.6471249461174011\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 4, loss is 0.6229302287101746\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 5, loss is 0.6734846234321594\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 6, loss is 0.658648669719696\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 7, loss is 0.6299156546592712\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 8, loss is 0.6276347041130066\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 9, loss is 0.631036102771759\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 10, loss is 0.6259282231330872\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 11, loss is 0.6630502343177795\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 12, loss is 0.6325421333312988\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 13, loss is 0.6309762597084045\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 14, loss is 0.6292407512664795\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 15, loss is 0.6763563752174377\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 6 step: 16, loss is 0.6419460773468018\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 17, loss is 0.6248297691345215\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 18, loss is 0.6298328042030334\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 19, loss is 0.6327269673347473\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 6 step: 20, loss is 0.6133233904838562\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 7 step: 1, loss is 0.6519748568534851\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 7 step: 2, loss is 0.6179152131080627\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 7 step: 3, loss is 0.6374312043190002\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 7 step: 4, loss is 0.6078339219093323\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 7 step: 5, loss is 0.6699704527854919\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 6, loss is 0.6531234383583069\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 7, loss is 0.6175227165222168\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 8, loss is 0.6163687109947205\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 9, loss is 0.6205224394798279\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 10, loss is 0.6138039231300354\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 11, loss is 0.6591354012489319\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 12, loss is 0.6221148371696472\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 13, loss is 0.6216248869895935\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 14, loss is 0.6186507344245911\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 15, loss is 0.6756906509399414\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 16, loss is 0.6339927315711975\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 17, loss is 0.6142431497573853\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 7 step: 18, loss is 0.6212851405143738\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 7 step: 19, loss is 0.6247808933258057\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 7 step: 20, loss is 0.6021566987037659\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 8 step: 1, loss is 0.6474187970161438\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 8 step: 2, loss is 0.6066238284111023\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 8 step: 3, loss is 0.6308358311653137\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 8 step: 4, loss is 0.5957546234130859\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 5, loss is 0.6676514744758606\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 6, loss is 0.6495341658592224\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 7, loss is 0.6084299087524414\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 8, loss is 0.6081578135490417\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 9, loss is 0.6128904223442078\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 10, loss is 0.6048380732536316\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 11, loss is 0.6559628844261169\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 12, loss is 0.6140362620353699\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 13, loss is 0.6142453551292419\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 14, loss is 0.6104624271392822\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 15, loss is 0.6745400428771973\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 16, loss is 0.6269956231117249\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 17, loss is 0.6063348650932312\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 8 step: 18, loss is 0.6145867109298706\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 8 step: 19, loss is 0.6182642579078674\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 8 step: 20, loss is 0.5933516621589661\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 1, loss is 0.6430105566978455\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 2, loss is 0.5988900065422058\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 3, loss is 0.6251896023750305\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 4, loss is 0.5872138142585754\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 5, loss is 0.6652453541755676\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 6, loss is 0.6448035836219788\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 7, loss is 0.6014544367790222\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 8, loss is 0.6002307534217834\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 9, loss is 0.6055696606636047\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 10, loss is 0.5978111624717712\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 11, loss is 0.6509959697723389\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 12, loss is 0.6079699993133545\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 13, loss is 0.6067505478858948\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 14, loss is 0.6042209267616272\n",
                        "acc is: 0.8571428571428571\n",
                        "epoch: 9 step: 15, loss is 0.6704697012901306\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 16, loss is 0.6214444041252136\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 17, loss is 0.60036700963974\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 18, loss is 0.6072891354560852\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 19, loss is 0.6111120581626892\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 9 step: 20, loss is 0.5859342217445374\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 1, loss is 0.637286365032196\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 2, loss is 0.5952162146568298\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 3, loss is 0.6180956363677979\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 4, loss is 0.5828344225883484\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 5, loss is 0.6614132523536682\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 6, loss is 0.63593989610672\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 7, loss is 0.5955978035926819\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 8, loss is 0.5902702212333679\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 9, loss is 0.5963718891143799\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 10, loss is 0.5917815566062927\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 11, loss is 0.6416452527046204\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 12, loss is 0.6034529805183411\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 13, loss is 0.5971806049346924\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 14, loss is 0.5993661284446716\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 15, loss is 0.661348283290863\n",
                        "acc is: 0.9047619047619048\n",
                        "epoch: 10 step: 16, loss is 0.6171853542327881\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 10 step: 17, loss is 0.59571373462677\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 10 step: 18, loss is 0.5975210666656494\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 10 step: 19, loss is 0.6016415357589722\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 10 step: 20, loss is 0.5791369080543518\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 1, loss is 0.6290397644042969\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 2, loss is 0.5960372090339661\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 3, loss is 0.6080538034439087\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 4, loss is 0.5827886462211609\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 5, loss is 0.6552587151527405\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 6, loss is 0.6209613680839539\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 7, loss is 0.5902404189109802\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 8, loss is 0.5767942667007446\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 9, loss is 0.583998441696167\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 10, loss is 0.5861452221870422\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 11, loss is 0.6263607144355774\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 12, loss is 0.5998499989509583\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 13, loss is 0.5843166708946228\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 14, loss is 0.5952101349830627\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 15, loss is 0.6462959051132202\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 16, loss is 0.6133195757865906\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 17, loss is 0.5917420387268066\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 18, loss is 0.5846152305603027\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 19, loss is 0.58916175365448\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 11 step: 20, loss is 0.5723044872283936\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 12 step: 1, loss is 0.6178817749023438\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 2, loss is 0.6007494330406189\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 3, loss is 0.5951125025749207\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 4, loss is 0.5859934687614441\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 12 step: 5, loss is 0.6468566060066223\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 12 step: 6, loss is 0.6001956462860107\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 12 step: 7, loss is 0.5850698351860046\n",
                        "acc is: 0.9523809523809523\n",
                        "epoch: 12 step: 8, loss is 0.5599423050880432\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 9, loss is 0.5686950087547302\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 10, loss is 0.5805995464324951\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 11, loss is 0.605900228023529\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 12, loss is 0.5963122844696045\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 13, loss is 0.5684507489204407\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 14, loss is 0.5909654498100281\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 15, loss is 0.6267394423484802\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 16, loss is 0.6084859371185303\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 17, loss is 0.5878047347068787\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 18, loss is 0.5696275234222412\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 19, loss is 0.5744984149932861\n",
                        "acc is: 1.0\n",
                        "epoch: 12 step: 20, loss is 0.5649637579917908\n",
                        "acc is: 1.0\n",
                        "[0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.6190476190476191, 0.6190476190476191, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7619047619047619, 0.7619047619047619, 0.8095238095238095, 0.8095238095238095, 0.8095238095238095, 0.8095238095238095, 0.8095238095238095, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9047619047619048, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 1.0, 1.0, 1.0, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 0.9523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
                    ]
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ]
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# -*- coding: utf-8 -*-\n",
                "\"\"\"\n",
                "Created on Mon Jun 13 17:01:29 2022\n",
                "\n",
                "@author: Waikikilick\n",
                "\"\"\"\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import mindspore as ms\n",
                "from mindquantum import *\n",
                "import mindspore.dataset as ds\n",
                "import matplotlib.pyplot as plt\n",
                "os.environ['OMP_NUM_THREADS'] = '2'\n",
                "import mindspore.context as context\n",
                "from mindspore import Model, load_checkpoint\n",
                "from mindspore.nn import Adam, Accuracy, SoftmaxCrossEntropyWithLogits\n",
                "from mindspore.train.callback import Callback, LossMonitor, ModelCheckpoint, CheckpointConfig\n",
                "\n",
                "ms.context.set_context(mode=ms.context.PYNATIVE_MODE, device_target=\"CPU\") #   \n",
                "ms.set_seed(1) # \n",
                "np.random.seed(1)\n",
                "\n",
                "class Main():\n",
                "    def __init__(self, batch_size=3): # \n",
                "        super().__init__()\n",
                "        # \n",
                "        self.train_x = np.load('./src/12_train_x.npy', allow_pickle=True)\n",
                "        self.train_y = np.load('./src/12_train_y.npy', allow_pickle=True)\n",
                "        self.eval_x = np.load('./src/12_eval_x.npy', allow_pickle=True)\n",
                "        self.eval_y = np.load('./src/12_eval_y.npy', allow_pickle=True)\n",
                "        self.batch_size = batch_size # \n",
                "        self.train_dataset = self.build_dataset(self.train_x, self.train_y, self.batch_size) # \n",
                "        self.eval_dataset = self.build_dataset(self.eval_x, self.eval_y, 21) # \n",
                "        self.qnet = MQLayer(self.build_grad_ops()) # \n",
                "        self.model = self.build_model() # \n",
                "        self.checkpoint_name = \"./model.ckpt\" # \n",
                "\n",
                "    def build_dataset(self, x, y, batch=None):\n",
                "        train = ds.NumpySlicesDataset(\n",
                "            {  \"image\": x.reshape((x.shape[0], -1)),\n",
                "                \"label\": y.astype(np.int32)\n",
                "            }, shuffle=False) # \n",
                "        if batch is not None:\n",
                "            train = train.batch(batch)\n",
                "        return train\n",
                "\n",
                "    def build_grad_ops(self):\n",
                "        # \n",
                "        qubit_num = 12\n",
                "        encoder = Circuit()\n",
                "        for i in range(qubit_num):\n",
                "            encoder += H.on(i)\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num-1):\n",
                "            encoder += ZZ(f'{0}').on([i, i+1])\n",
                "        encoder += ZZ(f'{0}').on([qubit_num-1, 0])\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num):\n",
                "            encoder += RX(f'{1}').on(i)\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num-1):\n",
                "            encoder += ZZ(f'{2}').on([i, i+1])\n",
                "        encoder += ZZ(f'{2}').on([qubit_num-1, 0])\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num):\n",
                "            encoder += RX(f'{3}').on(i)\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num-1):\n",
                "            encoder += ZZ(f'{4}').on([i, i+1])\n",
                "        encoder += ZZ(f'4').on([qubit_num-1, 0])\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num):\n",
                "            encoder += RX(f'{5}').on(i)\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num-1):\n",
                "            encoder += ZZ(f'{6}').on([i, i+1])\n",
                "        encoder += ZZ(f'{6}').on([qubit_num-1, 0])\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num):\n",
                "            encoder += RX(f'{7}').on(i)\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num-1):\n",
                "            encoder += ZZ(f'{8}').on([i, i+1])\n",
                "        encoder += ZZ(f'{8}').on([qubit_num-1, 0])\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num):\n",
                "            encoder += RX(f'{9}').on(i)\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num-1):\n",
                "            encoder += ZZ(f'{10}').on([i, i+1])\n",
                "        encoder += ZZ(f'{10}').on([qubit_num-1, 0])\n",
                "        encoder += BarrierGate()\n",
                "\n",
                "        for i in range(qubit_num):\n",
                "            encoder += RX(f'{11}').on(i)\n",
                "        encoder += BarrierGate()\n",
                "        encoder.no_grad() # \n",
                "\n",
                "        # \n",
                "        ansatz = Circuit()\n",
                "        ansatz += conv_circ('00',0,1)\n",
                "        ansatz += conv_circ('01',2,3)\n",
                "        ansatz += conv_circ('02',4,5)\n",
                "        ansatz += conv_circ('03',6,7)\n",
                "        ansatz += conv_circ('04',8,9)\n",
                "        ansatz += conv_circ('05',10,11)\n",
                "\n",
                "        ansatz += pool_circ('06',0,1)\n",
                "        ansatz += pool_circ('07',3,2)\n",
                "        ansatz += pool_circ('08',4,5)\n",
                "        ansatz += pool_circ('09',7,6)\n",
                "        ansatz += pool_circ('10',8,9)\n",
                "        ansatz += pool_circ('11',11,10)\n",
                "\n",
                "        ansatz += conv_circ('12',1,2)\n",
                "        ansatz += conv_circ('13',5,6)\n",
                "        ansatz += conv_circ('14',9,10)\n",
                "\n",
                "        ansatz += pool_circ('15',1,2)\n",
                "        ansatz += pool_circ('16',10,9)\n",
                "\n",
                "        ansatz += conv_circ('17',2,5)\n",
                "        ansatz += conv_circ('18',6,10)\n",
                "\n",
                "        ansatz += pool_circ('19',2,5)\n",
                "        ansatz += pool_circ('20',10,6)\n",
                "\n",
                "        ansatz += conv_circ('21',5,6)\n",
                "        ansatz += pool_circ('22',5,6)\n",
                "\n",
                "        total_circ = encoder.as_encoder() + ansatz.as_ansatz() #   + \n",
                "\n",
                "        ham = [Hamiltonian(QubitOperator(f'Z{i}')) for i in [5,6]] #  1,2 \n",
                "        sim = Simulator('mqvector', total_circ.n_qubits) # \n",
                "\n",
                "        grad_ops = sim.get_expectation_with_grad( # \n",
                "            ham,\n",
                "            total_circ,\n",
                "            parallel_worker=5)\n",
                "        return grad_ops\n",
                "\n",
                "    def build_model(self):\n",
                "        self.loss = ms.nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean') # \n",
                "        self.opti = ms.nn.Adam(self.qnet.trainable_params()) #  Adam \n",
                "        self.model = Model(self.qnet, self.loss, self.opti, metrics={'Acc': Accuracy()}) # \n",
                "        return self.model\n",
                "\n",
                "    def train(self, epoch=1): # \n",
                "        self.model.train(epoch, self.train_dataset, callbacks=[LossMonitor(1), acc, ckpoint_cb], dataset_sink_mode=False)\n",
                "\n",
                "    def export_trained_parameters(self): # \n",
                "        qnet_weight = self.qnet.weight.asnumpy()\n",
                "        ms.save_checkpoint(self.qnet, self.checkpoint_name)\n",
                "\n",
                "    def load_trained_parameters(self): # \n",
                "        ms.load_param_into_net(self.qnet,\n",
                "                               ms.load_checkpoint(self.checkpoint_name))\n",
                "\n",
                "    def load_training_ckpoint(self, ckpoint_name): # \n",
                "        ms.load_param_into_net(self.qnet,\n",
                "                               ms.load_checkpoint(ckpoint_name))\n",
                "\n",
                "    def predict(self, origin_test_x) -> float: # \n",
                "        test_x = origin_test_x.reshape((origin_test_x.shape[0], -1))\n",
                "        predict = self.model.predict(ms.Tensor(test_x))\n",
                "        predict = predict.asnumpy().flatten() > 0\n",
                "        return predict\n",
                "\n",
                "\n",
                "def conv_circ(prefix='0', bit_up=0, bit_down=1): # \n",
                "    _circ = Circuit()\n",
                "    _circ += RX('00').on(bit_up)\n",
                "    _circ += RY('01').on(bit_up)\n",
                "    _circ += RZ('02').on(bit_up)\n",
                "    _circ += RX('03').on(bit_down)\n",
                "    _circ += RY('04').on(bit_down)\n",
                "    _circ += RZ('05').on(bit_down)\n",
                "\n",
                "    _circ += ZZ('06').on([bit_up, bit_down])\n",
                "    _circ += YY('07').on([bit_up, bit_down])\n",
                "    _circ += XX('08').on([bit_up, bit_down])\n",
                "\n",
                "    _circ += RX('09').on(bit_up)\n",
                "    _circ += RY('10').on(bit_up)\n",
                "    _circ += RZ('11').on(bit_up)\n",
                "    _circ += RX('12').on(bit_down)\n",
                "    _circ += RY('13').on(bit_down)\n",
                "    _circ += RZ('14').on(bit_down)\n",
                "    _circ = add_prefix(_circ, prefix)\n",
                "    return _circ\n",
                "\n",
                "def pool_circ(prefix='0', bit_up=0, bit_down=1): # \n",
                "    _circ = Circuit()\n",
                "    _circ += RX('00').on(bit_up)\n",
                "    _circ += RY('01').on(bit_up)\n",
                "    _circ += RZ('02').on(bit_up)\n",
                "    _circ += RX('03').on(bit_down)\n",
                "    _circ += RY('04').on(bit_down)\n",
                "    _circ += RZ('05').on(bit_down)\n",
                "\n",
                "    _circ += X.on(bit_down, bit_up)\n",
                "\n",
                "    _circ += RX('06').on(bit_down)\n",
                "    _circ += RY('07').on(bit_down)\n",
                "    _circ += RZ('08').on(bit_down)\n",
                "    _circ = add_prefix(_circ, prefix)\n",
                "    return _circ\n",
                "\n",
                "class StepAcc(Callback):  # \n",
                "    def __init__(self, model):\n",
                "        self.model = model\n",
                "        self.acc = []\n",
                "\n",
                "    def step_end(self, run_context):\n",
                "        acc_tem = self.model.eval(main.eval_dataset, dataset_sink_mode=False)['Acc']\n",
                "        print('acc is:', acc_tem)\n",
                "        self.acc.append(acc_tem)\n",
                "\n",
                "if __name__ == '__main__':\n",
                "\n",
                "    main = Main()\n",
                "    main.ckpoint_file = './ckpoint_file'\n",
                "\n",
                "    acc = StepAcc(main.model)\n",
                "    config_ck = CheckpointConfig(save_checkpoint_steps=1, keep_checkpoint_max=100000000, exception_save=True)\n",
                "    ckpoint_cb = ModelCheckpoint(prefix='9b', directory=main.ckpoint_file, config=config_ck)\n",
                "    sys.stdout.flush()\n",
                "    epoch =  12\n",
                "    lr = 0.01\n",
                "    main.opti = ms.nn.Adam(main.qnet.trainable_params(), lr)\n",
                "    main.train(epoch) # \n",
                "    print(acc.acc) # \n",
                "\n",
                "    plt.plot(acc.acc) # \n",
                "    plt.title('Statistics of accuracy', fontsize=20)\n",
                "    plt.xlabel('Steps', fontsize=20)\n",
                "    plt.ylabel('Accuracy', fontsize=20)\n",
                "    plt.savefig('./src/result_12_qubit.png')\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "<table border=\"1\">\n",
                            "  <tr>\n",
                            "    <th>Software</th>\n",
                            "    <th>Version</th>\n",
                            "  </tr>\n",
                            "<tr><td>mindquantum</td><td>0.9.0</td></tr>\n",
                            "<tr><td>mindspore</td><td>2.1.0</td></tr>\n",
                            "<tr>\n",
                            "    <th>System</th>\n",
                            "    <th>Info</th>\n",
                            "</tr>\n",
                            "<tr><td>Python</td><td>3.9.7</td></tr><tr><td>OS</td><td>Linux x86_64</td></tr><tr><td>Memory</td><td>16.71 GB</td></tr><tr><td>CPU Max Thread</td><td>8</td></tr><tr><td>Date</td><td>Tue Oct 17 23:53:50 2023</td></tr>\n",
                            "</table>\n"
                        ],
                        "text/plain": [
                            "<mindquantum.utils.show_info.InfoTable at 0x7f570264d070>"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from mindquantum.utils.show_info import InfoTable\n",
                "\n",
                "InfoTable('mindquantum', 'mindspore')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        },
        "orig_nbformat": 2
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
